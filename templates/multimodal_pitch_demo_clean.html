<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="format-detection" content="telephone=no">
    <title>AI Pitch Practice - Multimodal Analysis</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    animation: {
                        'pulse-slow': 'pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite',
                        'bounce-slow': 'bounce 2s infinite',
                    }
                }
            }
        }
    </script>
    <style>
        .gradient-bg {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        .glass-effect {
            backdrop-filter: blur(10px);
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        .listening-pulse {
            animation: pulse 1.5s ease-in-out infinite;
            box-shadow: 0 0 20px rgba(34, 197, 94, 0.5);
        }
        .speaking-glow {
            animation: pulse 1s ease-in-out infinite;
            box-shadow: 0 0 20px rgba(239, 68, 68, 0.5);
        }
    </style>
</head>
<body class="gradient-bg min-h-screen">
    <!-- Main Container -->
    <div class="container mx-auto px-4 py-6 max-w-7xl">
        <!-- Header -->
        <div class="text-center mb-8">
            <h1 class="text-4xl font-bold text-white mb-2">üéØ AI Pitch Practice</h1>
            <p class="text-xl text-white/80">Multimodal Analysis with Voice & Video</p>
        </div>

        <!-- Main Content Grid -->
        <div class="grid lg:grid-cols-3 gap-6">
            <!-- Left Panel: Video & Controls -->
            <div class="lg:col-span-1">
                <!-- Video Preview -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4 flex items-center">
                        üìπ Video Preview
                    </h3>
                    <div class="relative">
                        <video id="videoPreview" autoplay muted playsinline 
                               class="w-full h-64 bg-gray-800 rounded-xl object-cover">
                        </video>
                        <div id="videoStatus" class="absolute top-2 right-2 px-3 py-1 rounded-full text-sm font-medium bg-gray-800/80 text-white">
                            Ready
                        </div>
                    </div>
                </div>

                <!-- Session Controls -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üéÆ Session Control</h3>
                    
                    <!-- Investor Persona -->
                    <div class="mb-4">
                        <label class="block text-white/80 text-sm font-medium mb-2">Investor Persona</label>
                        <select id="personaSelect" class="w-full px-4 py-3 bg-white/10 border border-white/20 rounded-xl text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent">
                            <option value="friendly">üòä Friendly Investor</option>
                            <option value="skeptical">ü§î Skeptical Investor</option>
                            <option value="technical">üî¨ Technical Investor</option>
                            <option value="aggressive">‚ö° Aggressive Investor</option>
                        </select>
                    </div>

                    <!-- Main Action Buttons -->
                    <div class="space-y-3">
                        <button id="start-btn" onclick="startSession()" 
                                class="w-full bg-green-500 hover:bg-green-600 text-white font-semibold py-4 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            üöÄ Start Practice Session
                        </button>
                        <button id="stop-btn" onclick="stopSession()" disabled
                                class="w-full bg-red-500 hover:bg-red-600 disabled:bg-gray-500 disabled:cursor-not-allowed text-white font-semibold py-4 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            ‚èπÔ∏è Stop Session
                        </button>
                        <button id="retry-connection-btn" onclick="retryConnection()" style="display: none;"
                                class="w-full bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            üîÑ Retry Connection
                        </button>
                        <button id="debug-btn" onclick="debugSystem()" 
                                class="w-full bg-purple-500 hover:bg-purple-600 text-white font-semibold py-2 px-4 rounded-xl transition-all duration-200 text-sm">
                            üîç Debug System
                        </button>
                        <button onclick="debugVideoAnalysis()" 
                                class="w-full bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-xl transition-all duration-200 text-sm">
                            üé• Debug Video Analysis
                        </button>
                    </div>
                </div>

                <!-- Audio Controls -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üîä Audio Settings</h3>
                    
                    <!-- TTS Toggle -->
                    <div class="flex items-center justify-between mb-4">
                        <span class="text-white/80">AI Voice Responses</span>
                        <label class="relative inline-flex items-center cursor-pointer">
                            <input type="checkbox" id="ttsEnabled" checked class="sr-only peer">
                            <div class="w-11 h-6 bg-gray-600 peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300 rounded-full peer peer-checked:after:translate-x-full peer-checked:after:border-white after:content-[''] after:absolute after:top-[2px] after:left-[2px] after:bg-white after:rounded-full after:h-5 after:w-5 after:transition-all peer-checked:bg-blue-600"></div>
                        </label>
                    </div>

                    <!-- TTS Mode Info -->
                    <div class="text-sm text-white/80 mb-4">
                        <span class="inline-flex items-center">
                            üåê <strong class="ml-1">Google Cloud TTS</strong>
                            <span class="ml-2 text-xs bg-green-600 px-2 py-1 rounded-full">Server-side</span>
                        </span>
                    </div>

                    <!-- Volume Control -->
                    <div class="mb-4">
                        <label class="block text-white/80 text-sm font-medium mb-2">
                            Volume: <span id="volumeDisplay">80%</span>
                        </label>
                        <input type="range" id="ttsVolume" min="0" max="100" value="80" 
                               class="w-full h-2 bg-gray-600 rounded-lg appearance-none cursor-pointer slider">
                    </div>

                    <!-- Quick Actions -->
                    <div class="grid grid-cols-2 gap-2 mb-3">
                        <button onclick="forceStartListening()" 
                                class="bg-blue-500 hover:bg-blue-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            üé§ Start Listening
                        </button>
                        <button onclick="testTTS()" 
                                class="bg-purple-500 hover:bg-purple-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            üîä Test TTS
                        </button>
                    </div>
                    <div class="grid grid-cols-2 gap-2">
                        <button onclick="debugAudioStreaming()" 
                                class="bg-orange-500 hover:bg-orange-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            üîç Debug Audio
                        </button>
                        <button onclick="testServerTTS()" 
                                class="bg-green-600 hover:bg-green-700 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            üåê Test Server TTS
                        </button>
                    </div>
                </div>
            </div>

            <!-- Middle Panel: Live Metrics -->
            <div class="lg:col-span-1">
                <!-- Status Indicators -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üìä Live Status</h3>
                    
                    <div class="space-y-4">
                        <!-- Connection Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">üîó Connection</span>
                            <div class="flex items-center">
                                <div id="connectionStatus" class="w-3 h-3 rounded-full bg-yellow-500 mr-2 animate-pulse"></div>
                                <span id="connectionStatusText" class="text-white text-sm font-medium">Connecting...</span>
                            </div>
                        </div>

                        <!-- Session Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">Session</span>
                            <div class="flex items-center">
                                <div id="sessionStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="sessionStatusText" class="text-white text-sm">Inactive</span>
                            </div>
                        </div>

                        <!-- Audio Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">Audio</span>
                            <div class="flex items-center">
                                <div id="audioStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="audioStatusText" class="text-white text-sm">Ready</span>
                            </div>
                        </div>

                        <!-- TTS Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">AI Voice</span>
                            <div class="flex items-center">
                                <div id="ttsStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="ttsStatusText" class="text-white text-sm">Ready</span>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Performance Metrics -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üìà Performance Metrics</h3>
                    
                    <div class="space-y-4">
                        <!-- Gesture Effectiveness (mapped to Eye Contact) -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="CVZone hand gesture analysis - effectiveness of hand movements and gestures">üëã Gesture Effectiveness</span>
                                <span id="eyeContactScore">70%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="eyeContactProgress" class="bg-gradient-to-r from-blue-500 to-blue-600 h-2 rounded-full transition-all duration-1000" style="width: 70%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">CVZone Hand Tracking</div>
                        </div>

                        <!-- Posture & Body Language -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="MediaPipe pose analysis - body posture and engagement level">üßç Posture & Body Language</span>
                                <span id="engagementScore">65%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="engagementProgress" class="bg-gradient-to-r from-green-500 to-green-600 h-2 rounded-full transition-all duration-1000" style="width: 65%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">MediaPipe Pose Analysis</div>
                        </div>

                        <!-- Facial Expression & Emotion -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="FER emotion recognition - facial expressions and emotional confidence">üòä Facial Expression & Emotion</span>
                                <span id="confidenceScore">60%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="confidenceProgress" class="bg-gradient-to-r from-yellow-500 to-yellow-600 h-2 rounded-full transition-all duration-1000" style="width: 60%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">FER Emotion Recognition</div>
                        </div>

                        <!-- Pitch Readiness -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="Combined analysis score - overall readiness for investor pitch">üöÄ Pitch Readiness</span>
                                <span id="clarityScore">70%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="clarityProgress" class="bg-gradient-to-r from-purple-500 to-purple-600 h-2 rounded-full transition-all duration-1000" style="width: 70%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">Enhanced AI Analysis</div>
                        </div>

                        <!-- Overall Enhanced Score -->
                        <div class="pt-2 border-t border-white/20">
                            <div class="flex justify-between text-white text-sm mb-1 font-semibold">
                                <span title="Combined score from CVZone, FER, and MediaPipe analysis">üèÜ Enhanced Analysis Score</span>
                                <span id="overallScore">66%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-3">
                                <div id="overallProgress" class="bg-gradient-to-r from-indigo-500 to-indigo-600 h-3 rounded-full transition-all duration-1000" style="width: 66%"></div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Enhanced Analysis Status -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üî¨ Enhanced Analysis Status</h3>
                    
                    <div class="space-y-3">
                        <!-- CVZone Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">üëã</span>
                                <span class="text-white/80 text-sm">CVZone Hand Tracking</span>
                            </div>
                            <div id="cvzoneStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- FER Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">üòä</span>
                                <span class="text-white/80 text-sm">FER Emotion Recognition</span>
                            </div>
                            <div id="ferStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- MediaPipe Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">üßç</span>
                                <span class="text-white/80 text-sm">MediaPipe Pose Analysis</span>
                            </div>
                            <div id="mediapipeStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- Analysis Summary -->
                        <div class="pt-2 border-t border-white/20">
                            <div class="text-xs text-white/60">
                                <div id="analysisStats" class="space-y-1">
                                    <div>Hands Detected: <span id="handsDetected">0</span></div>
                                    <div>Faces Detected: <span id="facesDetected">0</span></div>
                                    <div>Pose Detected: <span id="poseDetected">No</span></div>
                                    <div>Analysis Updates: <span id="analysisUpdates">0</span></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Real-time Enhanced Feedback -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üéØ Live Enhanced Feedback</h3>
                    
                    <div class="space-y-3">
                        <!-- Current Gesture -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Current Gesture</div>
                            <div id="currentGesture" class="text-white font-medium">üëã No gesture detected</div>
                        </div>

                        <!-- Current Emotion -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Facial Expression</div>
                            <div id="currentEmotion" class="text-white font-medium">üòê Neutral</div>
                        </div>

                        <!-- Current Posture -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Body Posture</div>
                            <div id="currentPosture" class="text-white font-medium">üßç Analyzing...</div>
                        </div>

                        <!-- Live Recommendations -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">AI Recommendation</div>
                            <div id="liveRecommendation" class="text-white/70 text-sm italic">Start your pitch to receive live feedback</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Right Panel: Conversation -->
            <div class="lg:col-span-1">
                <!-- Live Transcription -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üé§ Live Transcription</h3>
                    
                    <div class="bg-black/30 rounded-xl p-4 min-h-[120px]">
                        <div id="transcription-status" class="text-white/60 text-sm mb-2">Ready to listen...</div>
                        <div id="transcription-display" class="text-white text-base leading-relaxed">
                            <div class="text-white/40 italic">Your speech will appear here in real-time...</div>
                        </div>
                    </div>
                </div>

                <!-- Conversation Log -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">üí¨ Conversation</h3>
                    
                    <div id="conversation-log" class="bg-black/30 rounded-xl p-4 h-80 overflow-y-auto space-y-3">
                        <div class="text-white/40 italic text-center py-8">
                            <div class="mb-4">üöÄ Enhanced Video Analysis Ready</div>
                            <div class="text-sm space-y-1">
                                <div>üëã Hand Gesture Recognition</div>
                                <div>üòä Facial Emotion Analysis</div>
                                <div>üßç Body Pose Detection</div>
                            </div>
                            <div class="mt-4 text-white/60">Start a session to begin your pitch practice...</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Socket.IO -->
    <script src="https://cdn.socket.io/4.7.2/socket.io.min.js"></script>
    <!-- Enhanced Hybrid VAD System with TTS -->
    <script src="/static/js/enhanced-hybrid-vad.js" onload="console.log('‚úÖ Enhanced Hybrid VAD script loaded')" onerror="console.error('‚ùå Failed to load Enhanced Hybrid VAD script')"></script>
    
    <script>
        // Debug: Check if dependencies are loaded
        console.log('üîç Checking dependencies...');
        console.log('Socket.IO available:', typeof io !== 'undefined');
        console.log('EnhancedHybridVAD available:', typeof EnhancedHybridVAD !== 'undefined');
        
        // Global variables
        let enhancedVAD = null;
        let sessionId = null;
        let mediaStream = null;
        let videoAnalysisInterval = null;
        let currentAudio = null;
        let debugMode = true;

        // User activity tracking for metrics
        let userActivity = {
            isSpeaking: false,
            lastSpeechTime: 0,
            speechCount: 0
        };

        // Metrics tracking
        let previousMetrics = {
            eyeContact: 70,
            engagement: 65,
            confidence: 60,
            clarity: 70,
            overall: 66
        };

        // Video analysis state
        let videoAnalysisActive = false;
        let lastVideoAnalysisTime = 0;

        // Wait for all dependencies to load before initializing
        function waitForDependencies() {
            return new Promise((resolve) => {
                let attempts = 0;
                const maxAttempts = 50; // 5 seconds max wait
                
                const checkDependencies = () => {
                    attempts++;
                    console.log(`üîç Dependency check attempt ${attempts}/${maxAttempts}`);
                    console.log(`Socket.IO: ${typeof io !== 'undefined' ? '‚úÖ' : '‚ùå'}`);
                    console.log(`EnhancedHybridVAD: ${typeof EnhancedHybridVAD !== 'undefined' ? '‚úÖ' : '‚ùå'}`);
                    
                    if (typeof io !== 'undefined' && typeof EnhancedHybridVAD !== 'undefined') {
                        log('‚úÖ All dependencies loaded');
                        resolve();
                    } else if (attempts >= maxAttempts) {
                        log('‚ö†Ô∏è Timeout waiting for dependencies');
                        if (typeof io === 'undefined') {
                            throw new Error('Socket.IO failed to load');
                        }
                        if (typeof EnhancedHybridVAD === 'undefined') {
                            throw new Error('EnhancedHybridVAD failed to load');
                        }
                    } else {
                        log(`‚è≥ Waiting for dependencies... (${attempts}/${maxAttempts})`);
                        setTimeout(checkDependencies, 100);
                    }
                };
                checkDependencies();
            });
        }

        // Initialize on page load with dependency check
        document.addEventListener('DOMContentLoaded', async function() {
            log('üìÑ DOM Content Loaded - waiting for dependencies');
            try {
                await waitForDependencies();
                log('üöÄ Starting initialization with all dependencies ready');
                await initializeSystem();
            } catch (error) {
                log(`‚ùå DOM initialization failed: ${error.message}`, 'error');
                addMessage('system', `‚ùå Initialization failed: ${error.message}`);
                updateConnectionStatus('Failed', 'error');
                document.getElementById('retry-connection-btn').style.display = 'block';
            }
        });

        // Fallback initialization
        window.addEventListener('load', async function() {
            if (!enhancedVAD) {
                log('üîÑ Window load fallback triggered');
                try {
                    await waitForDependencies();
                    await initializeSystem();
                } catch (error) {
                    log(`‚ùå Fallback initialization failed: ${error.message}`, 'error');
                }
            }
        });

        // Additional fallback after a delay
        setTimeout(async () => {
            if (!enhancedVAD) {
                log('‚è∞ Final delayed fallback triggered');
                addMessage('system', '‚è∞ Attempting final initialization...');
                try {
                    await waitForDependencies();
                    await initializeSystem();
                } catch (error) {
                    log(`‚ùå Final fallback failed: ${error.message}`, 'error');
                    addMessage('system', '‚ùå All initialization attempts failed');
                    addMessage('system', 'üîß Please click "Debug System" to diagnose the issue');
                }
            }
        }, 5000);

        async function initializeSystem() {
            try {
                log('üöÄ Starting system initialization...');
                updateConnectionStatus('Starting...', 'warning');
                
                // Detect browser for Edge-specific handling
                const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
                if (isEdge) {
                    log('üåê Edge browser detected - using Edge-optimized initialization');
                    addMessage('system', 'üåê Microsoft Edge detected - optimizing for compatibility');
                }
                
                // Request microphone permission first
                log('üé§ Step 1: Requesting microphone permission...');
                await requestMicrophonePermission();
                
                log('üìπ Step 2: Initializing media...');
                await initializeMedia();
                
                log('üîß Step 3: Initializing EnhancedHybridVAD...');
                await initializeHybridVAD();
                
                log('‚öôÔ∏è Step 4: Setting up controls...');
                initializeControls();
                
                log('üìä Step 5: Setting up video analysis listeners...');
                setupVideoAnalysisEventListeners();
                
                // Start real-time metrics updates with Edge-specific timing
                log('üìà Step 6: Starting real-time metrics...');
                const metricsInterval = isEdge ? 4000 : 3000; // Slightly slower for Edge
                setInterval(updateRealtimeMetrics, metricsInterval);
                
                log('‚úÖ System initialized successfully');
                addMessage('system', '‚úÖ Enhanced Multimodal System Ready!');
                addMessage('system', 'üî¨ Professional CV libraries loaded and ready');
                if (isEdge) {
                    addMessage('system', 'üåê Edge browser optimizations applied');
                }
                addMessage('system', 'üéØ Click "Start Practice Session" to begin your enhanced pitch analysis!');
                
            } catch (error) {
                log(`‚ùå System initialization failed: ${error.message}`, 'error');
                console.error('System initialization error:', error);
                updateConnectionStatus('Failed', 'error');
                addMessage('system', `‚ùå System initialization failed: ${error.message}`);
                
                // Show retry button
                document.getElementById('retry-connection-btn').style.display = 'block';
                
                // Edge-specific error handling
                const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
                if (isEdge) {
                    addMessage('system', 'üåê Edge-specific troubleshooting:');
                    addMessage('system', '‚Ä¢ Ensure microphone permissions are granted');
                    addMessage('system', '‚Ä¢ Try refreshing the page');
                    addMessage('system', '‚Ä¢ Check if Edge is up to date');
                }
                
                // Show specific troubleshooting based on error
                if (error.message.includes('EnhancedHybridVAD')) {
                    addMessage('system', 'üí° Tip: Enhanced VAD system not loaded. Click "Retry Connection" to try again.');
                } else if (error.message.includes('microphone')) {
                    addMessage('system', 'üí° Tip: Please allow microphone access and click "Retry Connection"');
                } else if (error.message.includes('camera')) {
                    addMessage('system', 'üí° Tip: Camera access is optional. Click "Retry Connection" to continue.');
                } else {
                    addMessage('system', 'üîß Click "Retry Connection" to try again or refresh the page');
                }
            }
        }

        async function requestMicrophonePermission() {
            try {
                log('üé§ Requesting microphone permission...');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop());
                log('‚úÖ Microphone permission granted');
            } catch (error) {
                log(`‚ùå Microphone permission denied: ${error}`, 'error');
                throw new Error('Microphone permission required for speech recognition');
            }
        }

        async function initializeMedia() {
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 640, height: 480, facingMode: 'user' },
                    audio: false  // Audio handled by EnhancedHybridVAD
                });
                
                document.getElementById('videoPreview').srcObject = mediaStream;
                updateStatus('videoStatus', 'Ready', 'success');
                log('‚úÖ Video initialized');
                
            } catch (error) {
                log('‚ùå Video initialization failed - continuing without video', 'warn');
                updateStatus('videoStatus', 'No Camera', 'warning');
            }
        }

        async function initializeHybridVAD() {
            try {
                log('üîÑ Starting EnhancedHybridVAD initialization...');
                updateConnectionStatus('Initializing...', 'warning');
                
                // Check if EnhancedHybridVAD class is available
                if (typeof EnhancedHybridVAD === 'undefined') {
                    console.error('‚ùå EnhancedHybridVAD class not found');
                    console.log('Available globals:', Object.keys(window).filter(key => key.includes('Enhanced') || key.includes('VAD')));
                    
                    // Try to create a simple fallback
                    log('üîß Creating fallback VAD system...');
                    enhancedVAD = createFallbackVAD();
                    updateConnectionStatus('Fallback Mode', 'warning');
                    addMessage('system', '‚ö†Ô∏è Using fallback connection mode');
                    return;
                }
                
                log('‚úÖ EnhancedHybridVAD class found, creating instance...');
                
                enhancedVAD = new EnhancedHybridVAD({
                    debug: debugMode,
                    pauseThreshold: 3000,
                    minSpeechDuration: 500,
                    ttsEnabled: document.getElementById('ttsEnabled').checked,
                    ttsRate: 1.0,
                    ttsVolume: document.getElementById('ttsVolume').value / 100
                });
                
                log('‚úÖ EnhancedHybridVAD instance created, setting up callbacks...');
                
                // Set up callbacks
                enhancedVAD.setCallbacks({
                    onConnectionChange: handleConnectionChange,
                    onSessionStart: handleSessionStart,
                    onSessionEnd: handleSessionEnd,
                    onRealtimeTranscript: handleRealtimeTranscript,
                    onFinalTranscript: handleFinalTranscript,
                    onAIResponse: handleAIResponse,
                    onTTSStart: handleTTSStart,
                    onTTSEnd: handleTTSEnd,
                    onConversationStateChange: handleConversationStateChange,
                    onError: handleError,
                    onStatusChange: handleStatusChange
                });

                log('‚úÖ Callbacks set, initializing system...');
                updateConnectionStatus('Connecting...', 'warning');

                // Initialize the system with timeout
                const initPromise = enhancedVAD.initialize();
                const timeoutPromise = new Promise((_, reject) => {
                    setTimeout(() => reject(new Error('Initialization timeout after 10 seconds')), 10000);
                });
                
                const initialized = await Promise.race([initPromise, timeoutPromise]);
                
                if (initialized) {
                    updateConnectionStatus('Connected', 'success');
                    log('‚úÖ EnhancedHybridVAD initialized successfully');
                } else {
                    throw new Error('Failed to initialize EnhancedHybridVAD - initialize() returned false');
                }
                
            } catch (error) {
                log(`‚ùå EnhancedHybridVAD initialization failed: ${error.message}`, 'error');
                console.error('Full error details:', error);
                updateConnectionStatus('Failed', 'error');
                addMessage('system', `‚ùå Connection failed: ${error.message}`);
                
                // Show retry button
                document.getElementById('retry-connection-btn').style.display = 'block';
                throw error;
            }
        }

        function createFallbackVAD() {
            log('üîß Creating fallback VAD system...');
            return {
                isInitialized: false,
                isConnected: false,
                socket: null,
                callbacks: {},
                
                setCallbacks(callbacks) {
                    this.callbacks = callbacks;
                },
                
                async initialize() {
                    try {
                        if (typeof io === 'undefined') {
                            throw new Error('Socket.IO not available');
                        }
                        
                        this.socket = io();
                        
                        this.socket.on('connect', () => {
                            log('‚úÖ Fallback VAD connected');
                            this.isConnected = true;
                            this.isInitialized = true;
                            if (this.callbacks.onConnectionChange) {
                                this.callbacks.onConnectionChange({ connected: true });
                            }
                        });
                        
                        this.socket.on('disconnect', () => {
                            log('‚ùå Fallback VAD disconnected');
                            this.isConnected = false;
                            if (this.callbacks.onConnectionChange) {
                                this.callbacks.onConnectionChange({ connected: false });
                            }
                        });
                        
                        return true;
                    } catch (error) {
                        log(`‚ùå Fallback VAD initialization failed: ${error.message}`, 'error');
                        return false;
                    }
                },
                
                startSession() {
                    log('üîß Fallback VAD session start');
                    return Promise.resolve({ session_id: 'fallback-session' });
                },
                
                stopSession() {
                    log('üîß Fallback VAD session stop');
                    return Promise.resolve();
                }
            };
        }

        function initializeControls() {
            // TTS toggle
            const ttsToggle = document.getElementById('ttsEnabled');
            ttsToggle.addEventListener('change', function() {
                const enabled = this.checked;
                if (enhancedVAD) {
                    enhancedVAD.config.ttsEnabled = enabled;
                }
                log(`üîä TTS ${enabled ? 'enabled' : 'disabled'}`);
                addMessage('system', `üîä Voice responses ${enabled ? 'enabled' : 'disabled'}`);
                updateStatus('ttsStatusText', enabled ? 'Ready' : 'Disabled', enabled ? 'success' : 'warning');
            });

            // Volume control
            const volumeSlider = document.getElementById('ttsVolume');
            const volumeDisplay = document.getElementById('volumeDisplay');
            
            volumeSlider.addEventListener('input', function() {
                const volume = this.value;
                volumeDisplay.textContent = volume + '%';
                
                if (enhancedVAD) {
                    enhancedVAD.config.ttsVolume = volume / 100;
                }
            });
        }

        async function startSession() {
            if (!enhancedVAD) {
                alert('System not initialized');
                return;
            }
            
            try {
                sessionId = 'multimodal_session_' + Date.now();
                
                // Start enhanced VAD session
                await enhancedVAD.startSession(sessionId, document.getElementById('personaSelect').value);
                
                // Manually start listening after delay
                setTimeout(() => {
                    if (enhancedVAD && enhancedVAD.isRecording) {
                        log('üé§ Starting speech recognition...');
                        enhancedVAD.resumeListening();
                        
                        // Also ensure audio streaming is working
                        if (enhancedVAD.audioStreamingEnabled) {
                            log('üéµ Audio streaming should be active - speak to test');
                            addMessage('system', 'üéµ Audio streaming active - your voice is being processed');
                        }
                    }
                }, 2000);
                
                // Start video analysis if available
                if (enhancedVAD.socket) {
                    enhancedVAD.socket.emit('start_video_analysis', { session_id: sessionId });
                    videoAnalysisActive = true;
                }
                
                // Update UI
                document.getElementById('start-btn').disabled = true;
                document.getElementById('stop-btn').disabled = false;
                
                updateStatus('sessionStatusText', 'Active', 'success');
                updateStatus('audioStatusText', 'Listening', 'success');
                
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
                
                // Start video frame capture
                if (mediaStream) {
                    startVideoAnalysis();
                }
                
                addMessage('system', 'üöÄ Enhanced Multimodal Practice Session Started!');
                addMessage('system', 'üìπ Enhanced Video Analysis Active:');
                addMessage('system', '   üëã CVZone: Hand gesture recognition enabled');
                addMessage('system', '   üòä FER: Facial emotion analysis enabled');
                addMessage('system', '   üßç MediaPipe: Pose and body language tracking enabled');
                addMessage('system', 'üé§ Voice Activity Detection: Real-time speech processing');
                addMessage('system', 'ü§ñ AI Investor: Enhanced feedback with video insights');
                addMessage('system', 'üìä Live metrics updating based on your performance');
                addMessage('system', 'üéØ Begin your pitch - all systems are analyzing your presentation!');
                
                // Reset live feedback displays
                updateLiveFeedback('currentGesture', 'üëã Analyzing gestures...');
                updateLiveFeedback('currentEmotion', 'üòä Analyzing expressions...');
                updateLiveFeedback('currentPosture', 'üßç Analyzing posture...');
                updateLiveFeedback('liveRecommendation', 'üéØ Start speaking to receive AI recommendations');
                
                log('‚úÖ Enhanced session started successfully');
                
            } catch (error) {
                log(`‚ùå Failed to start session: ${error.message}`, 'error');
                addMessage('system', `‚ùå Failed to start session: ${error.message}`);
            }
        }

        async function stopSession() {
            if (!enhancedVAD) return;
            
            try {
                await enhancedVAD.stopSession();
                
                if (enhancedVAD.socket && sessionId) {
                    enhancedVAD.socket.emit('stop_video_analysis', { session_id: sessionId });
                    videoAnalysisActive = false;
                }
                
                // Stop video analysis
                if (videoAnalysisInterval) {
                    clearInterval(videoAnalysisInterval);
                    videoAnalysisInterval = null;
                }
                
                // Update UI
                document.getElementById('start-btn').disabled = false;
                document.getElementById('stop-btn').disabled = true;
                
                updateStatus('sessionStatusText', 'Inactive', 'default');
                updateStatus('audioStatusText', 'Ready', 'default');
                
                document.getElementById('transcription-status').textContent = 'Session ended';
                document.getElementById('transcription-status').className = 'text-white/60 text-sm mb-2';
                
                sessionId = null;
                
                // Reset live feedback displays
                updateLiveFeedback('currentGesture', 'üëã No gesture detected');
                updateLiveFeedback('currentEmotion', 'üòê Neutral');
                updateLiveFeedback('currentPosture', 'üßç Analyzing...');
                updateLiveFeedback('liveRecommendation', 'Start your pitch to receive live feedback');
                
                // Reset analysis status
                updateAnalysisStatus('cvzoneStatus', 'Ready', 'default');
                updateAnalysisStatus('ferStatus', 'Ready', 'default');
                updateAnalysisStatus('mediapipeStatus', 'Ready', 'default');
                
                addMessage('system', '‚èπÔ∏è Enhanced Practice Session Ended');
                addMessage('system', 'üìä Enhanced video analysis stopped');
                addMessage('system', 'üî¨ CVZone, FER, and MediaPipe analysis deactivated');
                addMessage('system', 'üìà Check your performance metrics above for insights');
                log('‚úÖ Enhanced session stopped');
                
            } catch (error) {
                log(`‚ùå Error stopping session: ${error.message}`, 'error');
            }
        }

        function startVideoAnalysis() {
            if (!mediaStream) return;
            
            videoAnalysisInterval = setInterval(() => {
                captureAndSendFrame();
            }, 2000); // Capture every 2 seconds
        }

        function captureAndSendFrame() {
            if (!sessionId || !enhancedVAD || !enhancedVAD.socket) {
                log('‚ö†Ô∏è Cannot send frame: missing sessionId, enhancedVAD, or socket', 'warning');
                return;
            }
            
            const video = document.getElementById('videoPreview');
            if (!video || video.readyState < 2) {
                log('‚ö†Ô∏è Video not ready for capture', 'warning');
                return;
            }
            
            try {
                // FIXED: Create a canvas with proper dimensions
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                
                // FIXED: Use actual video dimensions or fallback to 320x240
                canvas.width = video.videoWidth || 320;
                canvas.height = video.videoHeight || 240;
                
                // FIXED: Ensure video has content before drawing
                if (canvas.width === 0 || canvas.height === 0) {
                    canvas.width = 320;
                    canvas.height = 240;
                }
                
                // Draw the video frame to canvas
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                
                // FIXED: Compress more aggressively to reduce network load
                const frameData = canvas.toDataURL('image/jpeg', 0.7);
                
                // FIXED: Check if socket is connected before sending
                if (enhancedVAD.socket.connected) {
                    enhancedVAD.socket.emit('video_frame', {
                        session_id: sessionId,
                        frame_data: frameData
                    });
                    
                    // Update UI with frame size
                    const frameSizeKB = Math.round(frameData.length / 1024);
                    log(`üì∏ Frame sent: ${frameSizeKB}KB`, 'success');
                    
                    // FIXED: Update video status indicator
                    updateStatus('videoStatus', 'Analyzing', 'success');
                    
                    // FIXED: Force video_analysis_enabled flag to true
                    enhancedVAD.socket.emit('enable_video_analysis', {
                        session_id: sessionId,
                        enabled: true
                    });
                } else {
                    log('‚ö†Ô∏è Socket disconnected, cannot send frame', 'warning');
                    updateStatus('videoStatus', 'Disconnected', 'error');
                }
                
            } catch (error) {
                log(`‚ùå Error capturing frame: ${error.message}`, 'error');
                updateStatus('videoStatus', 'Error', 'error');
            }
        }

        // Event Handlers
        function handleConnectionChange(data) {
            updateConnectionStatus(data.connected ? 'Connected' : 'Disconnected', data.connected ? 'success' : 'error');
        }

        function handleSessionStart(data) {
            log(`üì° Session started: ${data.session_id}`);
        }

        function handleSessionEnd(data) {
            log(`üì° Session ended: ${data.session_id}`);
        }

        function handleRealtimeTranscript(data) {
            updateTranscriptionDisplay(data);
            userActivity.isSpeaking = true;
            userActivity.lastSpeechTime = Date.now();
            updateStatus('audioStatusText', 'Speaking', 'success');
        }

        function handleFinalTranscript(data) {
            log(`üìù Final transcript: ${data.transcript}`);
            addMessage('user', data.transcript);
            userActivity.speechCount++;
            updateStatus('audioStatusText', 'Processing', 'warning');
        }

        function handleAIResponse(data) {
            log(`ü§ñ AI response: ${data.message}`);
            addMessage('ai', data.message);
        }

        function handleTTSStart(data) {
            log('üîä AI started speaking');
            updateStatus('ttsStatusText', 'Speaking', 'success');
            updateStatus('audioStatusText', 'AI Speaking', 'warning');
        }

        function handleTTSEnd(data) {
            log('üîä AI finished speaking');
            updateStatus('ttsStatusText', 'Ready', 'success');
            updateStatus('audioStatusText', 'Listening', 'success');
        }

        function handleConversationStateChange(data) {
            const state = data && data.state ? data.state : 'unknown';
            log(`üîÑ Conversation state: ${state}`);
            
            // Update UI based on conversation state
            if (state === 'listening') {
                updateStatus('audioStatusText', 'Listening', 'success');
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
            } else if (state === 'processing') {
                updateStatus('audioStatusText', 'Processing', 'warning');
                document.getElementById('transcription-status').textContent = 'Processing your speech...';
                document.getElementById('transcription-status').className = 'text-yellow-400 text-sm mb-2';
            } else if (state === 'ai_speaking') {
                updateStatus('audioStatusText', 'AI Speaking', 'warning');
                document.getElementById('transcription-status').textContent = 'AI is responding...';
                document.getElementById('transcription-status').className = 'text-purple-400 text-sm mb-2';
            }
        }

        function handleError(data) {
            const error = data.error || data.message || 'Unknown error';
            
            // Don't show "aborted" errors to user as they're normal during state transitions
            if (error === 'aborted') {
                log(`üîÑ Speech recognition aborted (normal during state transitions)`);
                return;
            }
            
            log(`‚ùå Error: ${error}`, 'error');
            
            // Only show important errors to user
            if (error !== 'no-speech' && error !== 'audio-capture') {
                addMessage('system', `‚ùå Error: ${error}`);
            }
        }

        function handleStatusChange(data) {
            log(`üìä Status change: ${data.type}`);
        }

        // Video Analysis Event Listeners
        function setupVideoAnalysisEventListeners() {
            if (!enhancedVAD || !enhancedVAD.socket) {
                log('‚ö†Ô∏è Socket not available for video analysis events');
                return;
            }

            // Video analysis started
            enhancedVAD.socket.on('video_analysis_started', (data) => {
                log(`üìπ Enhanced video analysis started: ${data.analyzer_type}`);
                log(`üìä Session ID: ${data.session_id}`);
                addMessage('system', `üìπ ${data.message}`);
                
                // Set video analysis as active
                videoAnalysisActive = true;
                lastVideoAnalysisTime = Date.now();
                
                // Enhanced status display
                if (data.analyzer_type === 'enhanced') {
                    updateStatus('videoStatus', 'üöÄ Enhanced Analysis Active', 'success');
                } else {
                    updateStatus('videoStatus', `${data.analyzer_type} Active`, 'success');
                }
                
                // Show enhanced analysis capabilities
                if (data.analyzer_type === 'enhanced') {
                    addMessage('system', 'üöÄ Enhanced Video Analysis Active: CVZone hand tracking, FER emotion recognition, and MediaPipe pose analysis ready!');
                    addMessage('system', 'üéØ Real-time metrics will update based on your gestures, emotions, and body language!');
                    
                    // Show detailed capabilities
                    setTimeout(() => {
                        addMessage('system', 'üëã CVZone: Advanced hand gesture recognition and effectiveness scoring');
                        addMessage('system', 'üòä FER: Facial emotion recognition with pitch suitability analysis');
                        addMessage('system', 'üßç MediaPipe: Professional pose analysis and engagement detection');
                    }, 1000);
                } else {
                    addMessage('system', 'üéØ Real-time video analysis active - metrics will update based on your actual performance!');
                }
            });

            // Video analysis stopped
            enhancedVAD.socket.on('video_analysis_stopped', (data) => {
                log('üìπ Video analysis stopped');
                addMessage('system', 'üìπ Video analysis stopped');
                updateStatus('videoStatus', 'Ready', 'default');
            });

            // Real-time video analysis updates
            enhancedVAD.socket.on('video_analysis_update', (data) => {
                handleVideoAnalysisUpdate(data);
            });

            // Video insights
            enhancedVAD.socket.on('video_insights', (data) => {
                handleVideoInsights(data);
            });

            // Video errors
            enhancedVAD.socket.on('video_error', (data) => {
                log(`‚ùå Video error: ${data.error}`, 'error');
                addMessage('system', `‚ùå Video error: ${data.error}`);
            });

            log('‚úÖ Video analysis event listeners set up');
        }

        function handleVideoAnalysisUpdate(data) {
            try {
                log(`üìä Video analysis update received for session: ${data.session_id || 'unknown'}`);
                
                // Update video analysis state
                videoAnalysisActive = true;
                lastVideoAnalysisTime = Date.now();
                
                if (!analysis) {
                    log('‚ö†Ô∏è No analysis data in video update', 'warning');
                    return;
                }
                
                log(`üìà Analysis data: hands=${analysis.hand_analysis?.hands_detected || 0}, emotion=${analysis.emotion_analysis?.dominant_emotion || 'none'}, pose=${analysis.pose_analysis?.pose_detected || false}`);
                
                const analysis = data.analysis;
                
                // Handle enhanced video analysis structure
                const handAnalysis = analysis.hand_analysis || {};
                const emotionAnalysis = analysis.emotion_analysis || {};
                const poseAnalysis = analysis.pose_analysis || {};
                const overallScores = analysis.overall_scores || {};
                
                if (data.analyzer_type === 'enhanced') {
                    // Handle enhanced video analysis data with correct field names
                    const handAnalysis = analysis.hand_analysis || {};
                    const emotionAnalysis = analysis.emotion_analysis || {};
                    const poseAnalysis = analysis.pose_analysis || {};
                    const overallScores = analysis.overall_scores || {};

                    // Update metrics with enhanced video analysis scores
                    // Map enhanced scores to UI metrics (gesture_score -> eye contact, etc.)
                    const gestureScore = Math.round((overallScores.gesture_score || 0.7) * 100);
                    const emotionScore = Math.round((overallScores.emotion_score || 0.75) * 100);
                    const postureScore = Math.round((overallScores.posture_score || 0.8) * 100);
                    const pitchReadiness = Math.round((overallScores.pitch_readiness || 0.72) * 100);
                    
                    // Update individual metrics with enhanced data
                    updateMetricScore('eyeContact', gestureScore); // Gesture effectiveness as eye contact proxy
                    updateMetricScore('engagement', postureScore); // Posture score as engagement
                    updateMetricScore('confidence', emotionScore); // Emotion score as confidence
                    updateMetricScore('clarity', pitchReadiness); // Pitch readiness as clarity
                    
                    // Calculate overall score from enhanced metrics
                    const overallScore = Math.round((gestureScore + emotionScore + postureScore + pitchReadiness) / 4);
                    updateMetricScore('overall', overallScore);

                    // Enhanced logging with detailed CVZone, FER, and MediaPipe data
                    if (handAnalysis.hands_detected > 0) {
                        const gestures = handAnalysis.gestures || [];
                        const gestureTypes = gestures.map(g => g.type || 'unknown').join(', ');
                        const effectiveness = Math.round((handAnalysis.gesture_effectiveness || 0) * 100);
                        log(`üëã CVZone Hand Analysis: ${handAnalysis.hands_detected} hands detected, Gestures: [${gestureTypes}], Effectiveness: ${effectiveness}%`);
                        
                        // Update CVZone status
                        updateAnalysisStatus('cvzoneStatus', 'Active', 'success');
                        updateAnalysisStats('handsDetected', handAnalysis.hands_detected);
                        
                        // Update live gesture feedback
                        const gestureIcon = getGestureIcon(gestureTypes);
                        updateLiveFeedback('currentGesture', `${gestureIcon} ${gestureTypes || 'Hand detected'} (${effectiveness}% effective)`);
                    } else {
                        updateLiveFeedback('currentGesture', 'üëã No gesture detected');
                    }
                    
                    if (emotionAnalysis.faces_detected > 0) {
                        const emotion = emotionAnalysis.dominant_emotion || 'neutral';
                        const confidence = Math.round((emotionAnalysis.confidence_score || 0) * 100);
                        const suitability = emotionAnalysis.pitch_suitability || 'moderate';
                        log(`üòä FER Emotion Analysis: ${emotion} (${confidence}% confidence), Pitch Suitability: ${suitability}`);
                        
                        // Update FER status
                        updateAnalysisStatus('ferStatus', `${emotion} (${confidence}%)`, 'success');
                        updateAnalysisStats('facesDetected', emotionAnalysis.faces_detected);
                        
                        // Update live emotion feedback
                        const emotionIcon = getEmotionIcon(emotion);
                        const suitabilityColor = getSuitabilityColor(suitability);
                        updateLiveFeedback('currentEmotion', `${emotionIcon} ${emotion} (${confidence}%) - ${suitability} for pitch`, suitabilityColor);
                    } else {
                        updateLiveFeedback('currentEmotion', 'üòê No face detected');
                    }
                    
                    if (poseAnalysis.pose_detected) {
                        const engagement = poseAnalysis.engagement_level || 'neutral';
                        const posture = Math.round((poseAnalysis.posture_score || 0) * 100);
                        log(`üßç MediaPipe Pose Analysis: Engagement: ${engagement}, Posture Score: ${posture}%`);
                        
                        // Update MediaPipe status
                        updateAnalysisStatus('mediapipeStatus', `${engagement}`, 'success');
                        updateAnalysisStats('poseDetected', 'Yes');
                        
                        // Update live posture feedback
                        const postureIcon = getPostureIcon(engagement);
                        const engagementColor = getEngagementColor(engagement);
                        updateLiveFeedback('currentPosture', `${postureIcon} ${engagement} posture (${posture}% score)`, engagementColor);
                    } else {
                        updateLiveFeedback('currentPosture', 'üßç No pose detected');
                    }
                    
                    // Update analysis counter
                    let currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    updateAnalysisStats('analysisUpdates', currentUpdates + 1);

                    // Log and display AI recommendations from enhanced analysis
                    if (analysis.recommendations && analysis.recommendations.length > 0) {
                        const topRecommendation = analysis.recommendations[0];
                        log(`üí° Enhanced AI Recommendations: ${analysis.recommendations.slice(0, 2).join(', ')}`);
                        updateLiveFeedback('liveRecommendation', `üí° ${topRecommendation}`, 'text-blue-400');
                        
                        // Show recommendation in conversation occasionally
                        currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                        if (currentUpdates > 0 && currentUpdates % 15 === 0) {
                            addMessage('system', `üí° Enhanced AI Tip: ${topRecommendation}`);
                        }
                    }

                    // Comprehensive enhanced score logging
                    log(`üìä Enhanced Video Analysis - Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Pitch Readiness: ${pitchReadiness}%, Overall: ${overallScore}%`);
                    
                    // Show periodic enhanced analysis summary in conversation
                    currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    if (currentUpdates > 0 && currentUpdates % 10 === 0) {
                        addMessage('system', `üìä Enhanced Analysis Update #${currentUpdates}: Overall Score ${overallScore}% (Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Readiness: ${pitchReadiness}%)`);
                    }
                    
                    // Show periodic enhanced analysis summary in conversation
                    currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    if (currentUpdates > 0 && currentUpdates % 10 === 0) {
                        addMessage('system', `üìä Enhanced Analysis Update #${currentUpdates}: Overall Score ${overallScore}% (Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Readiness: ${pitchReadiness}%)`);
                    }

                } else {
                    // Handle basic video analysis data
                    const hands = analysis.hands || {};
                    const face = analysis.face || {};
                    const pose = analysis.pose || {};

                    if (face.eye_contact_score !== undefined) {
                        updateMetricScore('eyeContact', Math.round(face.eye_contact_score * 100));
                    }
                    
                    if (pose.engagement_level) {
                        const engagementMap = {
                            'highly_engaged': 90,
                            'engaged': 70,
                            'neutral': 50,
                            'disengaged': 30
                        };
                        updateMetricScore('engagement', engagementMap[pose.engagement_level] || 50);
                    }
                }

            } catch (error) {
                log(`‚ùå Error processing video analysis update: ${error.message}`, 'error');
            }
        }

        function handleVideoInsights(data) {
            const insights = data.insights || [];
            
            insights.forEach(insight => {
                // Enhanced insight categorization with appropriate icons
                let insightIcon = 'üí°';
                let insightPrefix = 'Insight';
                
                if (insight.toLowerCase().includes('gesture') || insight.toLowerCase().includes('hand')) {
                    insightIcon = 'üëã';
                    insightPrefix = 'CVZone Hand Analysis';
                } else if (insight.toLowerCase().includes('emotion') || insight.toLowerCase().includes('expression') || insight.toLowerCase().includes('facial')) {
                    insightIcon = 'üòä';
                    insightPrefix = 'FER Emotion Analysis';
                } else if (insight.toLowerCase().includes('posture') || insight.toLowerCase().includes('pose') || insight.toLowerCase().includes('body')) {
                    insightIcon = 'üßç';
                    insightPrefix = 'MediaPipe Pose Analysis';
                } else if (insight.toLowerCase().includes('confidence')) {
                    insightIcon = 'üí™';
                    insightPrefix = 'Confidence Analysis';
                } else if (insight.toLowerCase().includes('engagement')) {
                    insightIcon = 'üéØ';
                    insightPrefix = 'Engagement Analysis';
                } else if (insight.toLowerCase().includes('pitch') || insight.toLowerCase().includes('presentation')) {
                    insightIcon = 'üöÄ';
                    insightPrefix = 'Pitch Analysis';
                }
                
                log(`${insightIcon} Enhanced ${insightPrefix}: ${insight}`);
                addMessage('system', `${insightIcon} ${insightPrefix}: ${insight}`);
            });
            
            // Log analysis source for debugging
            if (data.analyzer_type === 'enhanced') {
                log('üî¨ Enhanced insights generated using professional CV libraries');
            }
        }

        function updateAnalysisStatus(elementId, status, type) {
            const element = document.getElementById(elementId);
            if (element) {
                element.textContent = status;
                element.className = `px-2 py-1 rounded-full text-xs font-medium ${getStatusClass(type)}`;
            }
        }

        function updateAnalysisStats(statId, value) {
            const element = document.getElementById(statId);
            if (element) {
                element.textContent = value;
            }
        }

        function getStatusClass(type) {
            switch(type) {
                case 'success': return 'bg-green-600 text-white';
                case 'warning': return 'bg-yellow-600 text-white';
                case 'error': return 'bg-red-600 text-white';
                default: return 'bg-gray-600 text-white';
            }
        }

        function updateLiveFeedback(elementId, text, colorClass = '') {
            const element = document.getElementById(elementId);
            if (element) {
                element.textContent = text;
                if (colorClass) {
                    element.className = `text-white font-medium ${colorClass}`;
                }
            }
        }

        function getGestureIcon(gestureType) {
            if (!gestureType) return 'üëã';
            const type = gestureType.toLowerCase();
            if (type.includes('point')) return 'üëâ';
            if (type.includes('open')) return '‚úã';
            if (type.includes('fist')) return '‚úä';
            if (type.includes('peace')) return '‚úåÔ∏è';
            if (type.includes('thumbs')) return 'üëç';
            return 'üëã';
        }

        function getEmotionIcon(emotion) {
            switch(emotion?.toLowerCase()) {
                case 'happy': return 'üòä';
                case 'confident': return 'üòé';
                case 'excited': return 'üòÉ';
                case 'focused': return 'üßê';
                case 'calm': return 'üòå';
                case 'surprised': return 'üòÆ';
                case 'concerned': return 'üòü';
                case 'nervous': return 'üò∞';
                case 'angry': return 'üò†';
                case 'sad': return 'üò¢';
                default: return 'üòê';
            }
        }

        function getPostureIcon(engagement) {
            switch(engagement?.toLowerCase()) {
                case 'highly_engaged': return 'üöÄ';
                case 'engaged': return 'üí™';
                case 'neutral': return 'üßç';
                case 'disengaged': return 'üò¥';
                default: return 'üßç';
            }
        }

        function getSuitabilityColor(suitability) {
            switch(suitability?.toLowerCase()) {
                case 'excellent': return 'text-green-400';
                case 'good': return 'text-blue-400';
                case 'moderate': return 'text-yellow-400';
                case 'needs_improvement': return 'text-red-400';
                default: return 'text-white';
            }
        }

        function getEngagementColor(engagement) {
            switch(engagement?.toLowerCase()) {
                case 'highly_engaged': return 'text-green-400';
                case 'engaged': return 'text-blue-400';
                case 'neutral': return 'text-yellow-400';
                case 'disengaged': return 'text-red-400';
                default: return 'text-white';
            }
        }

        function updateMetricScore(metricType, score) {
            // Clamp score between 0 and 100
            score = Math.max(0, Math.min(100, score));
            
            const scoreElement = document.getElementById(`${metricType}Score`);
            const progressElement = document.getElementById(`${metricType}Progress`);
            
            if (scoreElement) {
                scoreElement.textContent = `${score}%`;
            }
            
            if (progressElement) {
                progressElement.style.width = `${score}%`;
                
                // Update color based on score
                let colorClass = 'bg-gradient-to-r ';
                if (score >= 80) {
                    colorClass += 'from-green-500 to-green-600';
                } else if (score >= 60) {
                    colorClass += 'from-yellow-500 to-yellow-600';
                } else if (score >= 40) {
                    colorClass += 'from-orange-500 to-orange-600';
                } else {
                    colorClass += 'from-red-500 to-red-600';
                }
                
                // Keep the existing color scheme for specific metrics
                if (metricType === 'eyeContact') {
                    colorClass = 'bg-gradient-to-r from-blue-500 to-blue-600';
                } else if (metricType === 'engagement') {
                    colorClass = 'bg-gradient-to-r from-green-500 to-green-600';
                } else if (metricType === 'confidence') {
                    colorClass = 'bg-gradient-to-r from-yellow-500 to-yellow-600';
                } else if (metricType === 'clarity') {
                    colorClass = 'bg-gradient-to-r from-purple-500 to-purple-600';
                } else if (metricType === 'overall') {
                    colorClass = 'bg-gradient-to-r from-indigo-500 to-indigo-600';
                }
                
                progressElement.className = `${colorClass} h-2 rounded-full transition-all duration-1000`;
                if (metricType === 'overall') {
                    progressElement.className = `${colorClass} h-3 rounded-full transition-all duration-1000`;
                }
            }
        }

        // UI Helper Functions
        function updateStatus(elementId, text, type = 'default') {
            const element = document.getElementById(elementId);
            if (!element) return;
            
            element.textContent = text;
            
            // Update status indicator if it exists
            const statusIndicator = element.previousElementSibling;
            if (statusIndicator && statusIndicator.classList.contains('w-3')) {
                let className = 'w-3 h-3 rounded-full mr-2 ' + getStatusColor(type);
                
                // Add animation for connecting/loading states
                if (type === 'warning' && (text.includes('Connecting') || text.includes('Initializing') || text.includes('Starting'))) {
                    className += ' animate-pulse';
                }
                
                statusIndicator.className = className;
            }
        }

        function updateConnectionStatus(text, type) {
            updateStatus('connectionStatusText', text, type);
        }

        function getStatusColor(type) {
            switch (type) {
                case 'success': return 'bg-green-500';
                case 'warning': return 'bg-yellow-500';
                case 'error': return 'bg-red-500';
                default: return 'bg-gray-500';
            }
        }

        function updateTranscriptionDisplay(data) {
            const display = document.getElementById('transcription-display');
            const status = document.getElementById('transcription-status');
            
            if (data.interim || data.final) {
                const text = data.interim || data.final;
                display.innerHTML = `<div class="text-white">${text}</div>`;
                status.textContent = data.final ? 'Processing...' : 'Listening...';
                status.className = data.final ? 'text-yellow-400 text-sm mb-2' : 'text-green-400 text-sm mb-2 animate-pulse';
            }
        }

        function addMessage(sender, message, hasAudio = false) {
            const conversationLog = document.getElementById('conversation-log');
            const messageDiv = document.createElement('div');
            
            let messageClass = 'p-3 rounded-lg ';
            let messageContent = '';
            
            if (sender === 'system') {
                messageClass += 'bg-blue-500/20 border border-blue-500/30';
                messageContent = `<div class="text-blue-300 text-sm">${message}</div>`;
            } else if (sender === 'user') {
                messageClass += 'bg-green-500/20 border border-green-500/30 ml-8';
                messageContent = `<div class="text-green-300"><strong>You:</strong> ${message}</div>`;
            } else if (sender === 'ai') {
                messageClass += 'bg-purple-500/20 border border-purple-500/30 mr-8';
                const audioIcon = hasAudio ? ' üîä' : '';
                messageContent = `<div class="text-purple-300"><strong>AI Investor${audioIcon}:</strong> ${message}</div>`;
            }
            
            messageDiv.className = messageClass;
            messageDiv.innerHTML = messageContent;
            conversationLog.appendChild(messageDiv);
            conversationLog.scrollTop = conversationLog.scrollHeight;
        }

        function updateRealtimeMetrics() {
            const now = Date.now();
            const timeSinceLastSpeech = now - userActivity.lastSpeechTime;
            const isRecentlySpeaking = timeSinceLastSpeech < 5000;
            
            // Check if we have recent video analysis data (within last 10 seconds)
            const hasRecentVideoData = videoAnalysisActive && (now - lastVideoAnalysisTime < 10000);
            
            if (hasRecentVideoData) {
                // Don't override metrics if we have recent video analysis data
                log('üìä Using real video analysis data for metrics');
                return;
            }
            
            // Update metrics based on activity (fallback when no video data)
            const baseVariation = 8;
            
            // Eye contact
            const eyeContactTarget = isRecentlySpeaking ? 80 : 65;
            previousMetrics.eyeContact = smoothTransition(previousMetrics.eyeContact, eyeContactTarget, baseVariation, 40, 95);
            
            // Engagement
            const engagementTarget = isRecentlySpeaking ? 75 : 60;
            previousMetrics.engagement = smoothTransition(previousMetrics.engagement, engagementTarget, baseVariation, 30, 90);
            
            // Confidence
            const confidenceBonus = Math.min(userActivity.speechCount * 2, 15);
            const confidenceTarget = 55 + confidenceBonus + (isRecentlySpeaking ? 10 : 0);
            previousMetrics.confidence = smoothTransition(previousMetrics.confidence, confidenceTarget, 6, 35, 95);
            
            // Clarity
            const clarityTarget = 60 + Math.min(userActivity.speechCount * 1.5, 20);
            previousMetrics.clarity = smoothTransition(previousMetrics.clarity, clarityTarget, 8, 40, 90);
            
            // Overall
            previousMetrics.overall = (
                previousMetrics.eyeContact * 0.25 + 
                previousMetrics.engagement * 0.25 + 
                previousMetrics.confidence * 0.25 + 
                previousMetrics.clarity * 0.25
            );
            
            // Update UI
            updateMetricDisplay('eyeContact', previousMetrics.eyeContact);
            updateMetricDisplay('engagement', previousMetrics.engagement);
            updateMetricDisplay('confidence', previousMetrics.confidence);
            updateMetricDisplay('clarity', previousMetrics.clarity);
            updateMetricDisplay('overall', previousMetrics.overall);
        }

        function smoothTransition(current, target, variation, min, max) {
            const direction = target > current ? 1 : -1;
            const change = (Math.random() * variation) + (direction * variation * 0.3);
            const newValue = current + change;
            return Math.max(min, Math.min(max, newValue));
        }

        function updateMetricDisplay(metric, value) {
            const progressBar = document.getElementById(metric + 'Progress');
            const scoreText = document.getElementById(metric + 'Score');
            
            if (progressBar && scoreText) {
                progressBar.style.width = value + '%';
                scoreText.textContent = Math.round(value) + '%';
            }
        }

        // Quick Action Functions
        function forceStartListening() {
            if (!enhancedVAD || !sessionId) {
                addMessage('system', '‚ùå Start a session first');
                return;
            }
            
            try {
                enhancedVAD.resumeListening();
                updateStatus('audioStatusText', 'Listening', 'success');
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
                addMessage('system', 'üé§ Speech recognition started');
            } catch (error) {
                addMessage('system', `‚ùå Error: ${error.message}`);
            }
        }

        function testTTS() {
            const testMessage = "Hello! This is a test of the text-to-speech system. I'm ready to help you practice your pitch!";
            
            if (enhancedVAD && enhancedVAD.config.ttsEnabled) {
                enhancedVAD.speakAIResponse(testMessage);
                addMessage('system', 'üß™ Testing TTS - you should hear audio now');
            } else {
                addMessage('system', '‚ùå TTS is disabled or system not ready');
            }
        }

        function debugAudioStreaming() {
            log('üîç Audio Streaming Debug Info:');
            
            if (!enhancedVAD) {
                log('‚ùå EnhancedVAD not initialized');
                addMessage('system', '‚ùå EnhancedVAD not initialized');
                return;
            }
            
            log(`üéµ Audio Streaming Status:`);
            log(`   - Streaming Enabled: ${enhancedVAD.audioStreamingEnabled}`);
            log(`   - Audio Context: ${enhancedVAD.audioContext ? enhancedVAD.audioContext.state : 'null'}`);
            log(`   - Media Stream: ${enhancedVAD.mediaStream ? 'active' : 'null'}`);
            log(`   - Audio Processor: ${enhancedVAD.audioProcessor ? 'connected' : 'null'}`);
            log(`   - Audio Chunks Sent: ${enhancedVAD.audioChunkCount || 0}`);
            log(`   - Session ID: ${enhancedVAD.currentSessionId || 'none'}`);
            log(`   - Is Recording: ${enhancedVAD.isRecording}`);
            log(`   - AI Speaking: ${enhancedVAD.isAISpeaking}`);
            
            if (enhancedVAD.mediaStream) {
                const tracks = enhancedVAD.mediaStream.getAudioTracks();
                log(`   - Audio Tracks: ${tracks.length}`);
                tracks.forEach((track, i) => {
                    log(`     Track ${i}: ${track.label} (${track.readyState})`);
                });
            }
            
            addMessage('system', 'üîç Audio streaming debug info logged to console');
            
            // Test audio context resume
            if (enhancedVAD.audioContext && enhancedVAD.audioContext.state === 'suspended') {
                log('üîß Attempting to resume audio context...');
                enhancedVAD.audioContext.resume().then(() => {
                    log('‚úÖ Audio context resumed');
                    addMessage('system', '‚úÖ Audio context resumed');
                }).catch(error => {
                    log(`‚ùå Failed to resume audio context: ${error}`);
                    addMessage('system', `‚ùå Failed to resume audio context: ${error.message}`);
                });
            }
        }

        function testAudioStreaming() {
            if (!enhancedVAD) {
                addMessage('system', '‚ùå System not initialized');
                return;
            }
            
            log('üß™ Testing audio streaming...');
            addMessage('system', 'üß™ Testing audio streaming - speak now!');
            
            // Force start audio streaming
            enhancedVAD.forceStartAudioStreaming().then(success => {
                if (success) {
                    log('‚úÖ Audio streaming test started');
                    addMessage('system', '‚úÖ Audio streaming ready - speak to test');
                    
                    // Monitor for 10 seconds
                    const startCount = enhancedVAD.audioChunkCount || 0;
                    setTimeout(() => {
                        const endCount = enhancedVAD.audioChunkCount || 0;
                        const chunksSent = endCount - startCount;
                        log(`üìä Audio chunks sent in 10 seconds: ${chunksSent}`);
                        addMessage('system', `üìä Audio chunks sent: ${chunksSent} (should be >0 if speaking)`);
                    }, 10000);
                } else {
                    log('‚ùå Audio streaming test failed');
                    addMessage('system', '‚ùå Audio streaming test failed');
                }
            });
        }

        function debugSystem() {
            log('üîç Running system debug...');
            addMessage('system', 'üîç System Debug Information:');
            
            // Browser detection
            const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
            const isChrome = navigator.userAgent.indexOf('Chrome') !== -1 && !isEdge;
            const isSafari = navigator.userAgent.indexOf('Safari') !== -1 && !isChrome && !isEdge;
            const isFirefox = navigator.userAgent.indexOf('Firefox') !== -1;
            
            addMessage('system', `üåê Browser: ${isEdge ? 'Microsoft Edge' : isChrome ? 'Chrome' : isSafari ? 'Safari' : isFirefox ? 'Firefox' : 'Unknown'}`);
            addMessage('system', `üì± User Agent: ${navigator.userAgent}`);
            
            // Check dependencies
            addMessage('system', `Socket.IO: ${typeof io !== 'undefined' ? '‚úÖ Loaded' : '‚ùå Not loaded'}`);
            addMessage('system', `EnhancedHybridVAD: ${typeof EnhancedHybridVAD !== 'undefined' ? '‚úÖ Loaded' : '‚ùå Not loaded'}`);
            
            // Check browser capabilities with Edge-specific details
            addMessage('system', `getUserMedia: ${navigator.mediaDevices && navigator.mediaDevices.getUserMedia ? '‚úÖ Available' : '‚ùå Not available'}`);
            addMessage('system', `WebRTC: ${window.RTCPeerConnection ? '‚úÖ Available' : '‚ùå Not available'}`);
            
            // Speech API with Edge-specific checks
            const hasSpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const hasWebkitSpeech = window.webkitSpeechRecognition;
            const hasStandardSpeech = window.SpeechRecognition;
            
            addMessage('system', `Web Speech API: ${hasSpeechRecognition ? '‚úÖ Available' : '‚ùå Not available'}`);
            if (isEdge) {
                addMessage('system', `  ‚Ä¢ Standard SpeechRecognition: ${hasStandardSpeech ? '‚úÖ' : '‚ùå'}`);
                addMessage('system', `  ‚Ä¢ Webkit SpeechRecognition: ${hasWebkitSpeech ? '‚úÖ' : '‚ùå'}`);
            }
            
            // TTS capabilities (Server-side Google Cloud TTS only)
            addMessage('system', `Text-to-Speech: üåê Google Cloud TTS (Server-side)`);
            
            // Audio Context with Edge-specific checks
            const hasAudioContext = window.AudioContext || window.webkitAudioContext;
            addMessage('system', `AudioContext: ${hasAudioContext ? '‚úÖ Available' : '‚ùå Not available'}`);
            if (isEdge) {
                addMessage('system', `  ‚Ä¢ Standard AudioContext: ${window.AudioContext ? '‚úÖ' : '‚ùå'}`);
                addMessage('system', `  ‚Ä¢ Webkit AudioContext: ${window.webkitAudioContext ? '‚úÖ' : '‚ùå'}`);
            }
            
            // Check current state
            addMessage('system', `Enhanced VAD Instance: ${enhancedVAD ? '‚úÖ Created' : '‚ùå Not created'}`);
            addMessage('system', `Session ID: ${sessionId || 'None'}`);
            
            // Video analysis specific debug
            addMessage('system', `Video Analysis Active: ${videoAnalysisActive ? '‚úÖ Active' : '‚ùå Inactive'}`);
            addMessage('system', `Last Video Analysis: ${lastVideoAnalysisTime ? new Date(lastVideoAnalysisTime).toLocaleTimeString() : '‚ùå Never'}`);
            addMessage('system', `Video Element Ready: ${document.getElementById('videoPreview')?.readyState >= 2 ? '‚úÖ Ready' : '‚ùå Not ready'}`);
            addMessage('system', `Video Analysis Interval: ${videoAnalysisInterval ? '‚úÖ Running' : '‚ùå Not running'}`);
            
            // Edge-specific recommendations
            if (isEdge) {
                addMessage('system', 'üåê Edge-specific recommendations:');
                addMessage('system', '‚Ä¢ Ensure Edge is updated to latest version');
                addMessage('system', '‚Ä¢ Check microphone permissions in Edge settings');
                addMessage('system', '‚Ä¢ Try clearing browser cache if issues persist');
                addMessage('system', '‚Ä¢ Disable ad blockers that might interfere with WebRTC');
            }
            
            // Log to console for detailed debugging
            console.log('üîç Debug Info:', {
                browser: isEdge ? 'Edge' : isChrome ? 'Chrome' : isSafari ? 'Safari' : isFirefox ? 'Firefox' : 'Unknown',
                io: typeof io,
                EnhancedHybridVAD: typeof EnhancedHybridVAD,
                enhancedVAD: enhancedVAD,
                sessionId: sessionId,
                videoAnalysisActive: videoAnalysisActive,
                lastVideoAnalysisTime: lastVideoAnalysisTime,
                mediaStream: mediaStream,
                userAgent: navigator.userAgent,
                location: window.location.href,
                speechAPIs: {
                    SpeechRecognition: !!window.SpeechRecognition,
                    webkitSpeechRecognition: !!window.webkitSpeechRecognition,
                    ttsMode: 'Google Cloud TTS (Server-side only)'
                },
                audioAPIs: {
                    AudioContext: !!window.AudioContext,
                    webkitAudioContext: !!window.webkitAudioContext,
                    getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)
                }
            });
        }
        
        // Add this function to test video analysis specifically
        function debugVideoAnalysis() {
            log('üé• Debugging video analysis...');
            addMessage('system', 'üé• Video Analysis Debug:');
            
            const video = document.getElementById('videoPreview');
            addMessage('system', `Video Element: ${video ? '‚úÖ Found' : '‚ùå Not found'}`);
            addMessage('system', `Video Ready State: ${video ? video.readyState : 'N/A'} (need ‚â•2)`);
            addMessage('system', `Video Dimensions: ${video ? `${video.videoWidth}x${video.videoHeight}` : 'N/A'}`);
            addMessage('system', `Media Stream: ${mediaStream ? '‚úÖ Active' : '‚ùå Not active'}`);
            addMessage('system', `Video Analysis Active: ${videoAnalysisActive ? '‚úÖ Active' : '‚ùå Inactive'}`);
            
            if (sessionId && enhancedVAD && enhancedVAD.socket) {
                addMessage('system', 'üß™ Testing frame capture...');
                try {
                    captureAndSendFrame();
                    addMessage('system', '‚úÖ Frame capture test completed - check server logs');
                } catch (error) {
                    addMessage('system', `‚ùå Frame capture test failed: ${error.message}`);
                }
            } else {
                addMessage('system', '‚ùå Cannot test frame capture: missing session or socket');
            }
        }

        async function startSession() {
            log('üöÄ Starting practice session...');
            addMessage('system', 'üöÄ Starting practice session...');
            
            if (!enhancedVAD) {
                addMessage('system', '‚ùå Enhanced VAD not initialized');
                return;
            }
            
            const persona = document.getElementById('personaSelect').value;
            
            // Generate session ID
            const newSessionId = 'session_' + Date.now() + '_' + Math.random().toString(36).substr(2, 9);
            
            try {
                // Start the session
                await enhancedVAD.startSession(newSessionId, persona);
                
                // Update global session ID
                sessionId = newSessionId;
                
                // Update UI
                document.getElementById('start-btn').disabled = true;
                document.getElementById('stop-btn').disabled = false;
                
                addMessage('system', `‚úÖ Session started with ${persona} investor persona`);
                addMessage('system', `üìã Session ID: ${newSessionId}`);
            } catch (error) {
                addMessage('system', `‚ùå Failed to start session: ${error.message}`);
                log(`Start session error: ${error.message}`, 'error');
            }
        }
        
        async function stopSession() {
            log('‚èπÔ∏è Stopping practice session...');
            addMessage('system', '‚èπÔ∏è Stopping practice session...');
            
            if (!enhancedVAD) {
                addMessage('system', '‚ùå Enhanced VAD not initialized');
                return;
            }
            
            try {
                // Stop the session
                await enhancedVAD.stopSession();
                
                // Clear global session ID
                sessionId = null;
                
                // Update UI
                document.getElementById('start-btn').disabled = false;
                document.getElementById('stop-btn').disabled = true;
                
                addMessage('system', '‚úÖ Session stopped');
            } catch (error) {
                addMessage('system', `‚ùå Failed to stop session: ${error.message}`);
                log(`Stop session error: ${error.message}`, 'error');
            }
        }

        function retryConnection() {
            log('üîÑ Retrying connection...');
            addMessage('system', 'üîÑ Retrying connection...');
            
            // Reset state
            enhancedVAD = null;
            sessionId = null;
            
            // Hide retry button
            document.getElementById('retry-connection-btn').style.display = 'none';
            
            // Retry initialization
            initializeSystem();
        }

        // Test functions for debugging
        function forceStartListening() {
            log('üé§ Force starting listening...');
            addMessage('system', 'üé§ Attempting to force start listening...');
            
            if (enhancedVAD && enhancedVAD.startListening) {
                enhancedVAD.startListening();
                addMessage('system', '‚úÖ Listening started');
            } else {
                addMessage('system', '‚ùå Enhanced VAD not available');
            }
        }



        function testTTS() {
            log('üåê Testing Google Cloud TTS (Server-side only)...');
            addMessage('system', 'üåê Testing Google Cloud TTS...');
            
        }

        function debugAudioStreaming() {
            log('üîç Debugging audio streaming...');
            addMessage('system', 'üîç Audio streaming debug info:');
            
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    addMessage('system', '‚úÖ Audio access granted');
                    stream.getTracks().forEach(track => track.stop());
                })
                .catch(error => {
                    addMessage('system', `‚ùå Audio access failed: ${error.message}`);
                });
        }

        async function testServerTTS() {
            log('üåê Testing server-side Google Cloud TTS...');
            addMessage('system', 'üåê Testing server-side Google Cloud TTS...');
            
            try {
                const response = await fetch('/api/tts/test?text=Hello! This is a test of the Google Cloud Text-to-Speech system.&persona=friendly');
                const data = await response.json();
                
                if (data.success && data.audio_data) {
                    addMessage('system', '‚úÖ Server TTS generated successfully');
                    addMessage('system', `üìä Audio size: ${data.audio_size} bytes`);
                    
                    // Play the server-generated audio
                    if (enhancedVAD && enhancedVAD.playServerTTSAudio) {
                        addMessage('system', 'üîä Playing server TTS audio...');
                        await enhancedVAD.playServerTTSAudio(data.audio_data, data.text);
                        addMessage('system', '‚úÖ Server TTS playback completed');
                    } else {
                        // Fallback: create audio element directly
                        const audioBytes = atob(data.audio_data);
                        const audioArray = new Uint8Array(audioBytes.length);
                        for (let i = 0; i < audioBytes.length; i++) {
                            audioArray[i] = audioBytes.charCodeAt(i);
                        }
                        
                        const audioBlob = new Blob([audioArray], { type: 'audio/mp3' });
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        
                        audio.onended = () => {
                            URL.revokeObjectURL(audioUrl);
                            addMessage('system', '‚úÖ Server TTS playback completed');
                        };
                        
                        audio.onerror = (error) => {
                            addMessage('system', `‚ùå Audio playback failed: ${error.message || 'Unknown error'}`);
                            URL.revokeObjectURL(audioUrl);
                        };
                        
                        addMessage('system', 'üîä Playing server TTS audio...');
                        await audio.play();
                    }
                } else {
                    addMessage('system', `‚ùå Server TTS failed: ${data.detail || 'Unknown error'}`);
                }
            } catch (error) {
                addMessage('system', `‚ùå Server TTS test failed: ${error.message}`);
                log(`Server TTS test error: ${error.message}`, 'error');
            }
        }

        function testAudioStreaming() {
            log('üéµ Testing audio streaming...');
            addMessage('system', 'üéµ Testing audio streaming...');
            
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    addMessage('system', '‚úÖ Audio stream test successful');
                    setTimeout(() => {
                        stream.getTracks().forEach(track => track.stop());
                        addMessage('system', 'üîá Audio stream test completed');
                    }, 2000);
                })
                .catch(error => {
                    addMessage('system', `‚ùå Audio stream test failed: ${error.message}`);
                });
        }

        function log(message, level = 'info') {
            console.log(`[MultimodalDemo] ${message}`);
        }

        // Initialize enhanced analysis information
        function showEnhancedAnalysisInfo() {
            addMessage('system', 'üöÄ Enhanced Multimodal Pitch Analysis Ready!');
            addMessage('system', 'üî¨ Professional Computer Vision Libraries Integrated:');
            addMessage('system', 'üëã CVZone: Advanced hand gesture recognition with effectiveness scoring');
            addMessage('system', 'üòä FER: Facial emotion recognition with pitch suitability analysis');
            addMessage('system', 'üßç MediaPipe: Professional pose analysis and engagement detection');
            addMessage('system', 'üìä Real-time metrics update based on your actual performance');
            addMessage('system', 'üéØ Live feedback shows current gestures, emotions, and posture');
            addMessage('system', 'üí° AI recommendations generated from video analysis insights');
            addMessage('system', 'üöÄ Click "Start Practice Session" to begin your enhanced pitch analysis!');
        }

        // Initialize on page load
        window.addEventListener('load', function() {
            setTimeout(showEnhancedAnalysisInfo, 1000);
        });

        // Cleanup on page unload
        window.addEventListener('beforeunload', function() {
            if (enhancedVAD) {
                enhancedVAD.cleanup();
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
            }
        });
    </script>
</body>
</html>