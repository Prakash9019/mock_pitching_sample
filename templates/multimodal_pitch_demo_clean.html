<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="format-detection" content="telephone=no">
    <title>AI Pitch Practice - Multimodal Analysis</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    animation: {
                        'pulse-slow': 'pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite',
                        'bounce-slow': 'bounce 2s infinite',
                    }
                }
            }
        }
    </script>
    <style>
        .gradient-bg {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        .glass-effect {
            backdrop-filter: blur(10px);
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        .listening-pulse {
            animation: pulse 1.5s ease-in-out infinite;
            box-shadow: 0 0 20px rgba(34, 197, 94, 0.5);
        }
        .speaking-glow {
            animation: pulse 1s ease-in-out infinite;
            box-shadow: 0 0 20px rgba(239, 68, 68, 0.5);
        }
    </style>
</head>
<body class="gradient-bg min-h-screen">
    <!-- Main Container -->
    <div class="container mx-auto px-4 py-6 max-w-7xl">
        <!-- Header -->
        <div class="text-center mb-8">
            <h1 class="text-4xl font-bold text-white mb-2">ğŸ¯ AI Pitch Practice</h1>
            <p class="text-xl text-white/80">Multimodal Analysis with Voice & Video</p>
        </div>

        <!-- Main Content Grid -->
        <div class="grid lg:grid-cols-3 gap-6">
            <!-- Left Panel: Video & Controls -->
            <div class="lg:col-span-1">
                <!-- Video Preview -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4 flex items-center">
                        ğŸ“¹ Video Preview
                    </h3>
                    <div class="relative">
                        <video id="videoPreview" autoplay muted playsinline 
                               class="w-full h-64 bg-gray-800 rounded-xl object-cover">
                        </video>
                        <div id="videoStatus" class="absolute top-2 right-2 px-3 py-1 rounded-full text-sm font-medium bg-gray-800/80 text-white">
                            Ready
                        </div>
                    </div>
                </div>

                <!-- Session Controls -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ® Session Control</h3>
                    
                    <!-- Investor Persona -->
                    <div class="mb-4">
                        <label class="block text-white/80 text-sm font-medium mb-2">Investor Persona</label>
                        <select id="personaSelect" class="w-full px-4 py-3 bg-white/10 border border-white/20 rounded-xl text-white focus:ring-2 focus:ring-blue-500 focus:border-transparent">
                            <option value="friendly">ğŸ˜Š Friendly Investor</option>
                            <option value="skeptical">ğŸ¤” Skeptical Investor</option>
                            <option value="technical">ğŸ”¬ Technical Investor</option>
                            <option value="aggressive">âš¡ Aggressive Investor</option>
                        </select>
                    </div>

                    <!-- Main Action Buttons -->
                    <div class="space-y-3">
                        <button id="start-btn" onclick="startSession()" 
                                class="w-full bg-green-500 hover:bg-green-600 text-white font-semibold py-4 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            ğŸš€ Start Practice Session
                        </button>
                        <button id="stop-btn" onclick="stopSession()" disabled
                                class="w-full bg-red-500 hover:bg-red-600 disabled:bg-gray-500 disabled:cursor-not-allowed text-white font-semibold py-4 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            â¹ï¸ Stop Session
                        </button>
                        <button id="retry-connection-btn" onclick="retryConnection()" style="display: none;"
                                class="w-full bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-xl transition-all duration-200 transform hover:scale-105 shadow-lg">
                            ğŸ”„ Retry Connection
                        </button>
                        <button id="debug-btn" onclick="debugSystem()" 
                                class="w-full bg-purple-500 hover:bg-purple-600 text-white font-semibold py-2 px-4 rounded-xl transition-all duration-200 text-sm">
                            ğŸ” Debug System
                        </button>
                        <button onclick="debugVideoAnalysis()" 
                                class="w-full bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-xl transition-all duration-200 text-sm">
                            ğŸ¥ Debug Video Analysis
                        </button>
                    </div>
                </div>

                <!-- Audio Controls -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ”Š Audio Settings</h3>
                    
                    <!-- TTS Toggle -->
                    <div class="flex items-center justify-between mb-4">
                        <span class="text-white/80">AI Voice Responses</span>
                        <label class="relative inline-flex items-center cursor-pointer">
                            <input type="checkbox" id="ttsEnabled" checked class="sr-only peer">
                            <div class="w-11 h-6 bg-gray-600 peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300 rounded-full peer peer-checked:after:translate-x-full peer-checked:after:border-white after:content-[''] after:absolute after:top-[2px] after:left-[2px] after:bg-white after:rounded-full after:h-5 after:w-5 after:transition-all peer-checked:bg-blue-600"></div>
                        </label>
                    </div>

                    <!-- TTS Mode Info -->
                    <div class="text-sm text-white/80 mb-4">
                        <span class="inline-flex items-center">
                            ğŸŒ <strong class="ml-1">Google Cloud TTS</strong>
                            <span class="ml-2 text-xs bg-green-600 px-2 py-1 rounded-full">Server-side</span>
                        </span>
                    </div>

                    <!-- Volume Control -->
                    <div class="mb-4">
                        <label class="block text-white/80 text-sm font-medium mb-2">
                            Volume: <span id="volumeDisplay">80%</span>
                        </label>
                        <input type="range" id="ttsVolume" min="0" max="100" value="80" 
                               class="w-full h-2 bg-gray-600 rounded-lg appearance-none cursor-pointer slider">
                    </div>

                    <!-- Quick Actions -->
                    <div class="grid grid-cols-2 gap-2 mb-3">
                        <button onclick="forceStartListening()" 
                                class="bg-blue-500 hover:bg-blue-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            ğŸ¤ Start Listening
                        </button>
                        <button onclick="testTTS()" 
                                class="bg-purple-500 hover:bg-purple-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            ğŸ”Š Test TTS
                        </button>
                    </div>
                    <div class="grid grid-cols-2 gap-2">
                        <button onclick="debugAudioStreaming()" 
                                class="bg-orange-500 hover:bg-orange-600 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            ğŸ” Debug Audio
                        </button>
                        <button onclick="testServerTTS()" 
                                class="bg-green-600 hover:bg-green-700 text-white font-medium py-2 px-4 rounded-lg transition-all duration-200 text-sm">
                            ğŸŒ Test Server TTS
                        </button>
                    </div>
                </div>
            </div>

            <!-- Middle Panel: Live Metrics -->
            <div class="lg:col-span-1">
                <!-- Status Indicators -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ“Š Live Status</h3>
                    
                    <div class="space-y-4">
                        <!-- Connection Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">ğŸ”— Connection</span>
                            <div class="flex items-center">
                                <div id="connectionStatus" class="w-3 h-3 rounded-full bg-yellow-500 mr-2 animate-pulse"></div>
                                <span id="connectionStatusText" class="text-white text-sm font-medium">Connecting...</span>
                            </div>
                        </div>

                        <!-- Session Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">Session</span>
                            <div class="flex items-center">
                                <div id="sessionStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="sessionStatusText" class="text-white text-sm">Inactive</span>
                            </div>
                        </div>

                        <!-- Audio Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">Audio</span>
                            <div class="flex items-center">
                                <div id="audioStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="audioStatusText" class="text-white text-sm">Ready</span>
                            </div>
                        </div>

                        <!-- TTS Status -->
                        <div class="flex items-center justify-between">
                            <span class="text-white/80">AI Voice</span>
                            <div class="flex items-center">
                                <div id="ttsStatus" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                                <span id="ttsStatusText" class="text-white text-sm">Ready</span>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Performance Metrics -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ“ˆ Performance Metrics</h3>
                    
                    <div class="space-y-4">
                        <!-- Gesture Effectiveness (mapped to Eye Contact) -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="CVZone hand gesture analysis - effectiveness of hand movements and gestures">ğŸ‘‹ Gesture Effectiveness</span>
                                <span id="eyeContactScore">70%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="eyeContactProgress" class="bg-gradient-to-r from-blue-500 to-blue-600 h-2 rounded-full transition-all duration-1000" style="width: 70%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">CVZone Hand Tracking</div>
                        </div>

                        <!-- Posture & Body Language -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="MediaPipe pose analysis - body posture and engagement level">ğŸ§ Posture & Body Language</span>
                                <span id="engagementScore">65%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="engagementProgress" class="bg-gradient-to-r from-green-500 to-green-600 h-2 rounded-full transition-all duration-1000" style="width: 65%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">MediaPipe Pose Analysis</div>
                        </div>

                        <!-- Facial Expression & Emotion -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="FER emotion recognition - facial expressions and emotional confidence">ğŸ˜Š Facial Expression & Emotion</span>
                                <span id="confidenceScore">60%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="confidenceProgress" class="bg-gradient-to-r from-yellow-500 to-yellow-600 h-2 rounded-full transition-all duration-1000" style="width: 60%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">FER Emotion Recognition</div>
                        </div>

                        <!-- Pitch Readiness -->
                        <div>
                            <div class="flex justify-between text-white/80 text-sm mb-1">
                                <span title="Combined analysis score - overall readiness for investor pitch">ğŸš€ Pitch Readiness</span>
                                <span id="clarityScore">70%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-2">
                                <div id="clarityProgress" class="bg-gradient-to-r from-purple-500 to-purple-600 h-2 rounded-full transition-all duration-1000" style="width: 70%"></div>
                            </div>
                            <div class="text-xs text-white/60 mt-1">Enhanced AI Analysis</div>
                        </div>

                        <!-- Overall Enhanced Score -->
                        <div class="pt-2 border-t border-white/20">
                            <div class="flex justify-between text-white text-sm mb-1 font-semibold">
                                <span title="Combined score from CVZone, FER, and MediaPipe analysis">ğŸ† Enhanced Analysis Score</span>
                                <span id="overallScore">66%</span>
                            </div>
                            <div class="w-full bg-gray-600 rounded-full h-3">
                                <div id="overallProgress" class="bg-gradient-to-r from-indigo-500 to-indigo-600 h-3 rounded-full transition-all duration-1000" style="width: 66%"></div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Enhanced Analysis Status -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ”¬ Enhanced Analysis Status</h3>
                    
                    <div class="space-y-3">
                        <!-- CVZone Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">ğŸ‘‹</span>
                                <span class="text-white/80 text-sm">CVZone Hand Tracking</span>
                            </div>
                            <div id="cvzoneStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- FER Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">ğŸ˜Š</span>
                                <span class="text-white/80 text-sm">FER Emotion Recognition</span>
                            </div>
                            <div id="ferStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- MediaPipe Status -->
                        <div class="flex items-center justify-between">
                            <div class="flex items-center space-x-2">
                                <span class="text-lg">ğŸ§</span>
                                <span class="text-white/80 text-sm">MediaPipe Pose Analysis</span>
                            </div>
                            <div id="mediapipeStatus" class="px-2 py-1 rounded-full text-xs font-medium bg-gray-600 text-white">
                                Ready
                            </div>
                        </div>

                        <!-- Analysis Summary -->
                        <div class="pt-2 border-t border-white/20">
                            <div class="text-xs text-white/60">
                                <div id="analysisStats" class="space-y-1">
                                    <div>Hands Detected: <span id="handsDetected">0</span></div>
                                    <div>Faces Detected: <span id="facesDetected">0</span></div>
                                    <div>Pose Detected: <span id="poseDetected">No</span></div>
                                    <div>Analysis Updates: <span id="analysisUpdates">0</span></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Real-time Enhanced Feedback -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ¯ Live Enhanced Feedback</h3>
                    
                    <div class="space-y-3">
                        <!-- Current Gesture -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Current Gesture</div>
                            <div id="currentGesture" class="text-white font-medium">ğŸ‘‹ No gesture detected</div>
                        </div>

                        <!-- Current Emotion -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Facial Expression</div>
                            <div id="currentEmotion" class="text-white font-medium">ğŸ˜ Neutral</div>
                        </div>

                        <!-- Current Posture -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">Body Posture</div>
                            <div id="currentPosture" class="text-white font-medium">ğŸ§ Analyzing...</div>
                        </div>

                        <!-- Live Recommendations -->
                        <div class="bg-black/30 rounded-lg p-3">
                            <div class="text-white/80 text-sm mb-1">AI Recommendation</div>
                            <div id="liveRecommendation" class="text-white/70 text-sm italic">Start your pitch to receive live feedback</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Right Panel: Conversation -->
            <div class="lg:col-span-1">
                <!-- Live Transcription -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ¤ Live Transcription</h3>
                    
                    <div class="bg-black/30 rounded-xl p-4 min-h-[120px]">
                        <div id="transcription-status" class="text-white/60 text-sm mb-2">Ready to listen...</div>
                        <div id="transcription-display" class="text-white text-base leading-relaxed">
                            <div class="text-white/40 italic">Your speech will appear here in real-time...</div>
                        </div>
                    </div>
                </div>

                <!-- Conversation Log -->
                <div class="glass-effect rounded-2xl p-6 mb-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸ’¬ Conversation</h3>
                    
                    <div id="conversation-log" class="bg-black/30 rounded-xl p-4 h-80 overflow-y-auto space-y-3">
                        <div class="text-white/40 italic text-center py-8">
                            <div class="mb-4">ğŸš€ Enhanced Video Analysis Ready</div>
                            <div class="text-sm space-y-1">
                                <div>ğŸ‘‹ Hand Gesture Recognition</div>
                                <div>ğŸ˜Š Facial Emotion Analysis</div>
                                <div>ğŸ§ Body Pose Detection</div>
                            </div>
                            <div class="mt-4 text-white/60">Start a session to begin your pitch practice...</div>
                        </div>
                    </div>
                </div>

                <!-- Audio Conversation Recording -->
                <div class="glass-effect rounded-2xl p-6">
                    <h3 class="text-xl font-semibold text-white mb-4">ğŸµ Audio Recording</h3>
                    
                    <div class="bg-black/30 rounded-xl p-4">
                        <!-- Recording Status -->
                        <div class="flex items-center justify-between mb-4">
                            <div class="flex items-center space-x-2">
                                <div id="recording-indicator" class="w-3 h-3 rounded-full bg-gray-500"></div>
                                <span id="recording-status" class="text-white/80 text-sm">Ready to Record</span>
                            </div>
                            <div id="recording-duration" class="text-white/60 text-sm">00:00</div>
                        </div>
                        
                        <!-- Audio Info -->
                        <div class="grid grid-cols-2 gap-4 mb-4">
                            <div class="bg-white/5 rounded-lg p-3">
                                <div class="text-white/60 text-xs mb-1">User Audio</div>
                                <div id="user-segments" class="text-white font-medium">0 segments</div>
                                <div id="user-duration" class="text-white/80 text-sm">0.0s</div>
                            </div>
                            <div class="bg-white/5 rounded-lg p-3">
                                <div class="text-white/60 text-xs mb-1">AI Audio</div>
                                <div id="ai-segments" class="text-white font-medium">0 segments</div>
                                <div id="ai-duration" class="text-white/80 text-sm">0.0s</div>
                            </div>
                        </div>
                        
                        <!-- Audio Player (shown after session ends) -->
                        <div id="audio-player-container" class="hidden">
                            <div class="bg-white/5 rounded-lg p-4 mb-3">
                                <div class="flex items-center justify-between mb-2">
                                    <span class="text-white/80 text-sm">ğŸ“ Conversation Audio</span>
                                    <span id="audio-format" class="text-white/60 text-xs">WAV</span>
                                </div>
                                <audio id="conversation-audio" controls class="w-full">
                                    <source id="audio-source" src="" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                            
                            <!-- Audio Actions -->
                            <div class="flex space-x-2">
                                <button id="download-audio-btn" onclick="downloadAudio()" 
                                        class="flex-1 bg-blue-500/20 hover:bg-blue-500/30 text-blue-300 text-sm py-2 px-3 rounded-lg transition-colors">
                                    ğŸ“¥ Download
                                </button>
                                <button id="share-audio-btn" onclick="shareAudio()" 
                                        class="flex-1 bg-green-500/20 hover:bg-green-500/30 text-green-300 text-sm py-2 px-3 rounded-lg transition-colors">
                                    ğŸ”— Share Link
                                </button>
                                <button id="include-in-analysis-btn" onclick="toggleAudioInAnalysis()" 
                                        class="flex-1 bg-purple-500/20 hover:bg-purple-500/30 text-purple-300 text-sm py-2 px-3 rounded-lg transition-colors">
                                    ğŸ“Š Include in Analysis
                                </button>
                            </div>
                        </div>
                        
                        <!-- Recording Info -->
                        <div class="text-white/40 text-xs mt-3">
                            <div>ğŸ’¡ Audio conversations are automatically recorded and stored securely</div>
                            <div>ğŸ”’ Recordings include both your voice and AI responses for analysis</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Socket.IO -->
    <script src="https://cdn.socket.io/4.7.2/socket.io.min.js"></script>
    <!-- Enhanced Hybrid VAD System with TTS -->
    <script src="/static/js/enhanced-hybrid-vad.js" onload="console.log('âœ… Enhanced Hybrid VAD script loaded')" onerror="console.error('âŒ Failed to load Enhanced Hybrid VAD script')"></script>
    
    <script>
        // Debug: Check if dependencies are loaded
        console.log('ğŸ” Checking dependencies...');
        console.log('Socket.IO available:', typeof io !== 'undefined');
        console.log('EnhancedHybridVAD available:', typeof EnhancedHybridVAD !== 'undefined');
        
        // Global variables
        let enhancedVAD = null;
        let sessionId = null;
        let mediaStream = null;
        let videoAnalysisInterval = null;
        let currentAudio = null;
        let debugMode = true;

        // User activity tracking for metrics
        let userActivity = {
            isSpeaking: false,
            lastSpeechTime: 0,
            speechCount: 0
        };

        // Metrics tracking
        let previousMetrics = {
            eyeContact: 70,
            engagement: 65,
            confidence: 60,
            clarity: 70,
            overall: 66
        };

        // Video analysis state
        let videoAnalysisActive = false;
        let lastVideoAnalysisTime = 0;

        // Wait for all dependencies to load before initializing
        function waitForDependencies() {
            return new Promise((resolve) => {
                let attempts = 0;
                const maxAttempts = 50; // 5 seconds max wait
                
                const checkDependencies = () => {
                    attempts++;
                    console.log(`ğŸ” Dependency check attempt ${attempts}/${maxAttempts}`);
                    console.log(`Socket.IO: ${typeof io !== 'undefined' ? 'âœ…' : 'âŒ'}`);
                    console.log(`EnhancedHybridVAD: ${typeof EnhancedHybridVAD !== 'undefined' ? 'âœ…' : 'âŒ'}`);
                    
                    if (typeof io !== 'undefined' && typeof EnhancedHybridVAD !== 'undefined') {
                        log('âœ… All dependencies loaded');
                        resolve();
                    } else if (attempts >= maxAttempts) {
                        log('âš ï¸ Timeout waiting for dependencies');
                        if (typeof io === 'undefined') {
                            throw new Error('Socket.IO failed to load');
                        }
                        if (typeof EnhancedHybridVAD === 'undefined') {
                            throw new Error('EnhancedHybridVAD failed to load');
                        }
                    } else {
                        log(`â³ Waiting for dependencies... (${attempts}/${maxAttempts})`);
                        setTimeout(checkDependencies, 100);
                    }
                };
                checkDependencies();
            });
        }

        // Initialize on page load with dependency check
        document.addEventListener('DOMContentLoaded', async function() {
            log('ğŸ“„ DOM Content Loaded - waiting for dependencies');
            try {
                await waitForDependencies();
                log('ğŸš€ Starting initialization with all dependencies ready');
                await initializeSystem();
            } catch (error) {
                log(`âŒ DOM initialization failed: ${error.message}`, 'error');
                addMessage('system', `âŒ Initialization failed: ${error.message}`);
                updateConnectionStatus('Failed', 'error');
                document.getElementById('retry-connection-btn').style.display = 'block';
            }
        });

        // Fallback initialization
        window.addEventListener('load', async function() {
            if (!enhancedVAD) {
                log('ğŸ”„ Window load fallback triggered');
                try {
                    await waitForDependencies();
                    await initializeSystem();
                } catch (error) {
                    log(`âŒ Fallback initialization failed: ${error.message}`, 'error');
                }
            }
        });

        // Additional fallback after a delay
        setTimeout(async () => {
            if (!enhancedVAD) {
                log('â° Final delayed fallback triggered');
                addMessage('system', 'â° Attempting final initialization...');
                try {
                    await waitForDependencies();
                    await initializeSystem();
                } catch (error) {
                    log(`âŒ Final fallback failed: ${error.message}`, 'error');
                    addMessage('system', 'âŒ All initialization attempts failed');
                    addMessage('system', 'ğŸ”§ Please click "Debug System" to diagnose the issue');
                }
            }
        }, 5000);

        async function initializeSystem() {
            try {
                log('ğŸš€ Starting system initialization...');
                updateConnectionStatus('Starting...', 'warning');
                
                // Detect browser for Edge-specific handling
                const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
                if (isEdge) {
                    log('ğŸŒ Edge browser detected - using Edge-optimized initialization');
                    addMessage('system', 'ğŸŒ Microsoft Edge detected - optimizing for compatibility');
                }
                
                // Request microphone permission first
                log('ğŸ¤ Step 1: Requesting microphone permission...');
                await requestMicrophonePermission();
                
                log('ğŸ“¹ Step 2: Initializing media...');
                await initializeMedia();
                
                log('ğŸ”§ Step 3: Initializing EnhancedHybridVAD...');
                await initializeHybridVAD();
                
                log('âš™ï¸ Step 4: Setting up controls...');
                initializeControls();
                
                log('ğŸ“Š Step 5: Setting up video analysis listeners...');
                setupVideoAnalysisEventListeners();
                
                log('ğŸµ Step 5.5: Setting up audio conversation listeners...');
                setupAudioConversationEventListeners();
                
                // Start real-time metrics updates with Edge-specific timing
                log('ğŸ“ˆ Step 6: Starting real-time metrics...');
                const metricsInterval = isEdge ? 4000 : 3000; // Slightly slower for Edge
                setInterval(updateRealtimeMetrics, metricsInterval);
                
                log('âœ… System initialized successfully');
                addMessage('system', 'âœ… Enhanced Multimodal System Ready!');
                addMessage('system', 'ğŸ”¬ Professional CV libraries loaded and ready');
                if (isEdge) {
                    addMessage('system', 'ğŸŒ Edge browser optimizations applied');
                }
                addMessage('system', 'ğŸ¯ Click "Start Practice Session" to begin your enhanced pitch analysis!');
                
            } catch (error) {
                log(`âŒ System initialization failed: ${error.message}`, 'error');
                console.error('System initialization error:', error);
                updateConnectionStatus('Failed', 'error');
                addMessage('system', `âŒ System initialization failed: ${error.message}`);
                
                // Show retry button
                document.getElementById('retry-connection-btn').style.display = 'block';
                
                // Edge-specific error handling
                const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
                if (isEdge) {
                    addMessage('system', 'ğŸŒ Edge-specific troubleshooting:');
                    addMessage('system', 'â€¢ Ensure microphone permissions are granted');
                    addMessage('system', 'â€¢ Try refreshing the page');
                    addMessage('system', 'â€¢ Check if Edge is up to date');
                }
                
                // Show specific troubleshooting based on error
                if (error.message.includes('EnhancedHybridVAD')) {
                    addMessage('system', 'ğŸ’¡ Tip: Enhanced VAD system not loaded. Click "Retry Connection" to try again.');
                } else if (error.message.includes('microphone')) {
                    addMessage('system', 'ğŸ’¡ Tip: Please allow microphone access and click "Retry Connection"');
                } else if (error.message.includes('camera')) {
                    addMessage('system', 'ğŸ’¡ Tip: Camera access is optional. Click "Retry Connection" to continue.');
                } else {
                    addMessage('system', 'ğŸ”§ Click "Retry Connection" to try again or refresh the page');
                }
            }
        }

        async function requestMicrophonePermission() {
            try {
                log('ğŸ¤ Requesting microphone permission...');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop());
                log('âœ… Microphone permission granted');
            } catch (error) {
                log(`âŒ Microphone permission denied: ${error}`, 'error');
                throw new Error('Microphone permission required for speech recognition');
            }
        }

        async function initializeMedia() {
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 640, height: 480, facingMode: 'user' },
                    audio: false  // Audio handled by EnhancedHybridVAD
                });
                
                document.getElementById('videoPreview').srcObject = mediaStream;
                updateStatus('videoStatus', 'Ready', 'success');
                log('âœ… Video initialized');
                
            } catch (error) {
                log('âŒ Video initialization failed - continuing without video', 'warn');
                updateStatus('videoStatus', 'No Camera', 'warning');
            }
        }

        async function initializeHybridVAD() {
            try {
                log('ğŸ”„ Starting EnhancedHybridVAD initialization...');
                updateConnectionStatus('Initializing...', 'warning');
                
                // Check if EnhancedHybridVAD class is available
                if (typeof EnhancedHybridVAD === 'undefined') {
                    console.error('âŒ EnhancedHybridVAD class not found');
                    console.log('Available globals:', Object.keys(window).filter(key => key.includes('Enhanced') || key.includes('VAD')));
                    
                    // Try to create a simple fallback
                    log('ğŸ”§ Creating fallback VAD system...');
                    enhancedVAD = createFallbackVAD();
                    updateConnectionStatus('Fallback Mode', 'warning');
                    addMessage('system', 'âš ï¸ Using fallback connection mode');
                    return;
                }
                
                log('âœ… EnhancedHybridVAD class found, creating instance...');
                
                enhancedVAD = new EnhancedHybridVAD({
                    debug: debugMode,
                    pauseThreshold: 3000,
                    minSpeechDuration: 500,
                    ttsEnabled: document.getElementById('ttsEnabled').checked,
                    ttsRate: 1.0,
                    ttsVolume: document.getElementById('ttsVolume').value / 100
                });
                
                log('âœ… EnhancedHybridVAD instance created, setting up callbacks...');
                
                // Set up callbacks
                enhancedVAD.setCallbacks({
                    onConnectionChange: handleConnectionChange,
                    onSessionStart: handleSessionStart,
                    onSessionEnd: handleSessionEnd,
                    onRealtimeTranscript: handleRealtimeTranscript,
                    onFinalTranscript: handleFinalTranscript,
                    onAIResponse: handleAIResponse,
                    onTTSStart: handleTTSStart,
                    onTTSEnd: handleTTSEnd,
                    onConversationStateChange: handleConversationStateChange,
                    onError: handleError,
                    onStatusChange: handleStatusChange
                });

                log('âœ… Callbacks set, initializing system...');
                updateConnectionStatus('Connecting...', 'warning');

                // Initialize the system with timeout
                const initPromise = enhancedVAD.initialize();
                const timeoutPromise = new Promise((_, reject) => {
                    setTimeout(() => reject(new Error('Initialization timeout after 10 seconds')), 10000);
                });
                
                const initialized = await Promise.race([initPromise, timeoutPromise]);
                
                if (initialized) {
                    updateConnectionStatus('Connected', 'success');
                    log('âœ… EnhancedHybridVAD initialized successfully');
                } else {
                    throw new Error('Failed to initialize EnhancedHybridVAD - initialize() returned false');
                }
                
            } catch (error) {
                log(`âŒ EnhancedHybridVAD initialization failed: ${error.message}`, 'error');
                console.error('Full error details:', error);
                updateConnectionStatus('Failed', 'error');
                addMessage('system', `âŒ Connection failed: ${error.message}`);
                
                // Show retry button
                document.getElementById('retry-connection-btn').style.display = 'block';
                throw error;
            }
        }

        function createFallbackVAD() {
            log('ğŸ”§ Creating fallback VAD system...');
            return {
                isInitialized: false,
                isConnected: false,
                socket: null,
                callbacks: {},
                
                setCallbacks(callbacks) {
                    this.callbacks = callbacks;
                },
                
                async initialize() {
                    try {
                        if (typeof io === 'undefined') {
                            throw new Error('Socket.IO not available');
                        }
                        
                        this.socket = io();
                        
                        this.socket.on('connect', () => {
                            log('âœ… Fallback VAD connected');
                            this.isConnected = true;
                            this.isInitialized = true;
                            if (this.callbacks.onConnectionChange) {
                                this.callbacks.onConnectionChange({ connected: true });
                            }
                        });
                        
                        this.socket.on('disconnect', () => {
                            log('âŒ Fallback VAD disconnected');
                            this.isConnected = false;
                            if (this.callbacks.onConnectionChange) {
                                this.callbacks.onConnectionChange({ connected: false });
                            }
                        });
                        
                        return true;
                    } catch (error) {
                        log(`âŒ Fallback VAD initialization failed: ${error.message}`, 'error');
                        return false;
                    }
                },
                
                startSession() {
                    log('ğŸ”§ Fallback VAD session start');
                    return Promise.resolve({ session_id: 'fallback-session' });
                },
                
                stopSession() {
                    log('ğŸ”§ Fallback VAD session stop');
                    return Promise.resolve();
                }
            };
        }

        function initializeControls() {
            // TTS toggle
            const ttsToggle = document.getElementById('ttsEnabled');
            ttsToggle.addEventListener('change', function() {
                const enabled = this.checked;
                if (enhancedVAD) {
                    enhancedVAD.config.ttsEnabled = enabled;
                }
                log(`ğŸ”Š TTS ${enabled ? 'enabled' : 'disabled'}`);
                addMessage('system', `ğŸ”Š Voice responses ${enabled ? 'enabled' : 'disabled'}`);
                updateStatus('ttsStatusText', enabled ? 'Ready' : 'Disabled', enabled ? 'success' : 'warning');
            });

            // Volume control
            const volumeSlider = document.getElementById('ttsVolume');
            const volumeDisplay = document.getElementById('volumeDisplay');
            
            volumeSlider.addEventListener('input', function() {
                const volume = this.value;
                volumeDisplay.textContent = volume + '%';
                
                if (enhancedVAD) {
                    enhancedVAD.config.ttsVolume = volume / 100;
                }
            });
        }

        async function startSession() {
            if (!enhancedVAD) {
                alert('System not initialized');
                return;
            }
            
            try {
                sessionId = 'multimodal_session_' + Date.now();
                
                // Start enhanced VAD session
                await enhancedVAD.startSession(sessionId, document.getElementById('personaSelect').value);
                
                // Manually start listening after delay
                setTimeout(() => {
                    if (enhancedVAD && enhancedVAD.isRecording) {
                        log('ğŸ¤ Starting speech recognition...');
                        enhancedVAD.resumeListening();
                        
                        // Also ensure audio streaming is working
                        if (enhancedVAD.audioStreamingEnabled) {
                            log('ğŸµ Audio streaming should be active - speak to test');
                            addMessage('system', 'ğŸµ Audio streaming active - your voice is being processed');
                        }
                    }
                }, 2000);
                
                // Start video analysis if available
                if (enhancedVAD.socket) {
                    enhancedVAD.socket.emit('start_video_analysis', { session_id: sessionId });
                    videoAnalysisActive = true;
                }
                
                // Update UI
                document.getElementById('start-btn').disabled = true;
                document.getElementById('stop-btn').disabled = false;
                
                updateStatus('sessionStatusText', 'Active', 'success');
                updateStatus('audioStatusText', 'Listening', 'success');
                
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
                
                // Start video frame capture
                if (mediaStream) {
                    startVideoAnalysis();
                }
                
                addMessage('system', 'ğŸš€ Enhanced Multimodal Practice Session Started!');
                addMessage('system', 'ğŸ“¹ Enhanced Video Analysis Active:');
                addMessage('system', '   ğŸ‘‹ CVZone: Hand gesture recognition enabled');
                addMessage('system', '   ğŸ˜Š FER: Facial emotion analysis enabled');
                addMessage('system', '   ğŸ§ MediaPipe: Pose and body language tracking enabled');
                addMessage('system', 'ğŸ¤ Voice Activity Detection: Real-time speech processing');
                addMessage('system', 'ğŸ¤– AI Investor: Enhanced feedback with video insights');
                addMessage('system', 'ğŸ“Š Live metrics updating based on your performance');
                addMessage('system', 'ğŸ¯ Begin your pitch - all systems are analyzing your presentation!');
                
                // Start audio recording
                startAudioRecording();
                addMessage('system', 'ğŸµ Audio conversation recording started');
                
                // Reset live feedback displays
                updateLiveFeedback('currentGesture', 'ğŸ‘‹ Analyzing gestures...');
                updateLiveFeedback('currentEmotion', 'ğŸ˜Š Analyzing expressions...');
                updateLiveFeedback('currentPosture', 'ğŸ§ Analyzing posture...');
                updateLiveFeedback('liveRecommendation', 'ğŸ¯ Start speaking to receive AI recommendations');
                
                log('âœ… Enhanced session started successfully');
                
            } catch (error) {
                log(`âŒ Failed to start session: ${error.message}`, 'error');
                addMessage('system', `âŒ Failed to start session: ${error.message}`);
            }
        }

        async function stopSession() {
            if (!enhancedVAD) return;
            
            try {
                await enhancedVAD.stopSession();
                
                if (enhancedVAD.socket && sessionId) {
                    enhancedVAD.socket.emit('stop_video_analysis', { session_id: sessionId });
                    videoAnalysisActive = false;
                }
                
                // Stop video analysis
                if (videoAnalysisInterval) {
                    clearInterval(videoAnalysisInterval);
                    videoAnalysisInterval = null;
                }
                
                // Update UI
                document.getElementById('start-btn').disabled = false;
                document.getElementById('stop-btn').disabled = true;
                
                updateStatus('sessionStatusText', 'Inactive', 'default');
                updateStatus('audioStatusText', 'Ready', 'default');
                
                document.getElementById('transcription-status').textContent = 'Session ended';
                document.getElementById('transcription-status').className = 'text-white/60 text-sm mb-2';
                
                sessionId = null;
                
                // Reset live feedback displays
                updateLiveFeedback('currentGesture', 'ğŸ‘‹ No gesture detected');
                updateLiveFeedback('currentEmotion', 'ğŸ˜ Neutral');
                updateLiveFeedback('currentPosture', 'ğŸ§ Analyzing...');
                updateLiveFeedback('liveRecommendation', 'Start your pitch to receive live feedback');
                
                // Reset analysis status
                updateAnalysisStatus('cvzoneStatus', 'Ready', 'default');
                updateAnalysisStatus('ferStatus', 'Ready', 'default');
                updateAnalysisStatus('mediapipeStatus', 'Ready', 'default');
                
                addMessage('system', 'â¹ï¸ Enhanced Practice Session Ended');
                addMessage('system', 'ğŸ“Š Enhanced video analysis stopped');
                addMessage('system', 'ğŸ”¬ CVZone, FER, and MediaPipe analysis deactivated');
                addMessage('system', 'ğŸµ Processing audio conversation...');
                
                // Stop audio recording (will be updated with URL when available)
                stopAudioRecording();
                
                addMessage('system', 'ğŸ“ˆ Check your performance metrics above for insights');
                log('âœ… Enhanced session stopped');
                
            } catch (error) {
                log(`âŒ Error stopping session: ${error.message}`, 'error');
            }
        }

        function startVideoAnalysis() {
            if (!mediaStream) return;
            
            videoAnalysisInterval = setInterval(() => {
                captureAndSendFrame();
            }, 2000); // Capture every 2 seconds
        }

        function captureAndSendFrame() {
            if (!sessionId || !enhancedVAD || !enhancedVAD.socket) {
                log('âš ï¸ Cannot send frame: missing sessionId, enhancedVAD, or socket', 'warning');
                return;
            }
            
            const video = document.getElementById('videoPreview');
            if (!video || video.readyState < 2) {
                log('âš ï¸ Video not ready for capture', 'warning');
                return;
            }
            
            try {
                // FIXED: Create a canvas with proper dimensions
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                
                // FIXED: Use actual video dimensions or fallback to 320x240
                canvas.width = video.videoWidth || 320;
                canvas.height = video.videoHeight || 240;
                
                // FIXED: Ensure video has content before drawing
                if (canvas.width === 0 || canvas.height === 0) {
                    canvas.width = 320;
                    canvas.height = 240;
                }
                
                // Draw the video frame to canvas
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                
                // FIXED: Compress more aggressively to reduce network load
                const frameData = canvas.toDataURL('image/jpeg', 0.7);
                
                // FIXED: Check if socket is connected before sending
                if (enhancedVAD.socket.connected) {
                    enhancedVAD.socket.emit('video_frame', {
                        session_id: sessionId,
                        frame_data: frameData
                    });
                    
                    // Update UI with frame size
                    const frameSizeKB = Math.round(frameData.length / 1024);
                    log(`ğŸ“¸ Frame sent: ${frameSizeKB}KB`, 'success');
                    
                    // FIXED: Update video status indicator
                    updateStatus('videoStatus', 'Analyzing', 'success');
                    
                    // FIXED: Force video_analysis_enabled flag to true
                    enhancedVAD.socket.emit('enable_video_analysis', {
                        session_id: sessionId,
                        enabled: true
                    });
                } else {
                    log('âš ï¸ Socket disconnected, cannot send frame', 'warning');
                    updateStatus('videoStatus', 'Disconnected', 'error');
                }
                
            } catch (error) {
                log(`âŒ Error capturing frame: ${error.message}`, 'error');
                updateStatus('videoStatus', 'Error', 'error');
            }
        }

        // Event Handlers
        function handleConnectionChange(data) {
            updateConnectionStatus(data.connected ? 'Connected' : 'Disconnected', data.connected ? 'success' : 'error');
        }

        function handleSessionStart(data) {
            log(`ğŸ“¡ Session started: ${data.session_id}`);
        }

        function handleSessionEnd(data) {
            log(`ğŸ“¡ Session ended: ${data.session_id}`);
        }

        function handleRealtimeTranscript(data) {
            updateTranscriptionDisplay(data);
            userActivity.isSpeaking = true;
            userActivity.lastSpeechTime = Date.now();
            updateStatus('audioStatusText', 'Speaking', 'success');
        }

        function handleFinalTranscript(data) {
            log(`ğŸ“ Final transcript: ${data.transcript}`);
            addMessage('user', data.transcript);
            userActivity.speechCount++;
            updateStatus('audioStatusText', 'Processing', 'warning');
            
            // Update user audio segment count (estimate 3 seconds per segment)
            onUserAudioSegment(3.0);
        }

        function handleAIResponse(data) {
            log(`ğŸ¤– AI response: ${data.message}`);
            addMessage('ai', data.message);
            
            // Estimate AI audio duration based on text length (average 150 words per minute)
            const wordCount = data.message.split(' ').length;
            const estimatedDuration = (wordCount / 150) * 60; // Convert to seconds
            onAIAudioSegment(estimatedDuration);
        }

        function handleTTSStart(data) {
            log('ğŸ”Š AI started speaking');
            updateStatus('ttsStatusText', 'Speaking', 'success');
            updateStatus('audioStatusText', 'AI Speaking', 'warning');
        }

        function handleTTSEnd(data) {
            log('ğŸ”Š AI finished speaking');
            updateStatus('ttsStatusText', 'Ready', 'success');
            updateStatus('audioStatusText', 'Listening', 'success');
        }

        function handleConversationStateChange(data) {
            const state = data && data.state ? data.state : 'unknown';
            log(`ğŸ”„ Conversation state: ${state}`);
            
            // Update UI based on conversation state
            if (state === 'listening') {
                updateStatus('audioStatusText', 'Listening', 'success');
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
            } else if (state === 'processing') {
                updateStatus('audioStatusText', 'Processing', 'warning');
                document.getElementById('transcription-status').textContent = 'Processing your speech...';
                document.getElementById('transcription-status').className = 'text-yellow-400 text-sm mb-2';
            } else if (state === 'ai_speaking') {
                updateStatus('audioStatusText', 'AI Speaking', 'warning');
                document.getElementById('transcription-status').textContent = 'AI is responding...';
                document.getElementById('transcription-status').className = 'text-purple-400 text-sm mb-2';
            }
        }

        function handleError(data) {
            const error = data.error || data.message || 'Unknown error';
            
            // Don't show "aborted" errors to user as they're normal during state transitions
            if (error === 'aborted') {
                log(`ğŸ”„ Speech recognition aborted (normal during state transitions)`);
                return;
            }
            
            log(`âŒ Error: ${error}`, 'error');
            
            // Only show important errors to user
            if (error !== 'no-speech' && error !== 'audio-capture') {
                addMessage('system', `âŒ Error: ${error}`);
            }
        }

        function handleStatusChange(data) {
            log(`ğŸ“Š Status change: ${data.type}`);
        }

        // Video Analysis Event Listeners
        function setupVideoAnalysisEventListeners() {
            if (!enhancedVAD || !enhancedVAD.socket) {
                log('âš ï¸ Socket not available for video analysis events');
                return;
            }

            // Video analysis started
            enhancedVAD.socket.on('video_analysis_started', (data) => {
                log(`ğŸ“¹ Enhanced video analysis started: ${data.analyzer_type}`);
                log(`ğŸ“Š Session ID: ${data.session_id}`);
                addMessage('system', `ğŸ“¹ ${data.message}`);
                
                // Set video analysis as active
                videoAnalysisActive = true;
                lastVideoAnalysisTime = Date.now();
                
                // Enhanced status display
                if (data.analyzer_type === 'enhanced') {
                    updateStatus('videoStatus', 'ğŸš€ Enhanced Analysis Active', 'success');
                } else {
                    updateStatus('videoStatus', `${data.analyzer_type} Active`, 'success');
                }
                
                // Show enhanced analysis capabilities
                if (data.analyzer_type === 'enhanced') {
                    addMessage('system', 'ğŸš€ Enhanced Video Analysis Active: CVZone hand tracking, FER emotion recognition, and MediaPipe pose analysis ready!');
                    addMessage('system', 'ğŸ¯ Real-time metrics will update based on your gestures, emotions, and body language!');
                    
                    // Show detailed capabilities
                    setTimeout(() => {
                        addMessage('system', 'ğŸ‘‹ CVZone: Advanced hand gesture recognition and effectiveness scoring');
                        addMessage('system', 'ğŸ˜Š FER: Facial emotion recognition with pitch suitability analysis');
                        addMessage('system', 'ğŸ§ MediaPipe: Professional pose analysis and engagement detection');
                    }, 1000);
                } else {
                    addMessage('system', 'ğŸ¯ Real-time video analysis active - metrics will update based on your actual performance!');
                }
            });

            // Video analysis stopped
            enhancedVAD.socket.on('video_analysis_stopped', (data) => {
                log('ğŸ“¹ Video analysis stopped');
                addMessage('system', 'ğŸ“¹ Video analysis stopped');
                updateStatus('videoStatus', 'Ready', 'default');
            });

            // Real-time video analysis updates
            enhancedVAD.socket.on('video_analysis_update', (data) => {
                handleVideoAnalysisUpdate(data);
            });

            // Video insights
            enhancedVAD.socket.on('video_insights', (data) => {
                handleVideoInsights(data);
            });

            // Video errors
            enhancedVAD.socket.on('video_error', (data) => {
                log(`âŒ Video error: ${data.error}`, 'error');
                addMessage('system', `âŒ Video error: ${data.error}`);
            });

            log('âœ… Video analysis event listeners set up');
        }

        // Audio Conversation Event Listeners
        function setupAudioConversationEventListeners() {
            if (!enhancedVAD || !enhancedVAD.socket) {
                log('âš ï¸ Socket not available for audio conversation events');
                return;
            }

            // Audio conversation recording started
            enhancedVAD.socket.on('audio_recording_started', (data) => {
                log(`ğŸµ Audio recording started for session: ${data.session_id}`);
                addMessage('system', 'ğŸµ Audio conversation recording active');
                updateRecordingStatus('Recording', 'success');
            });

            // Audio conversation recording completed
            enhancedVAD.socket.on('audio_recording_completed', (data) => {
                log(`ğŸµ Audio recording completed: ${data.audio_url}`);
                addMessage('system', 'ğŸµ Audio conversation saved successfully');
                
                // Update UI with completed recording
                stopAudioRecording(data.audio_url);
                
                if (data.audio_info) {
                    // Update audio recording data with actual values
                    audioRecordingData = {
                        userSegments: data.audio_info.user_segments || audioRecordingData.userSegments,
                        aiSegments: data.audio_info.ai_segments || audioRecordingData.aiSegments,
                        userDuration: data.audio_info.user_duration || audioRecordingData.userDuration,
                        aiDuration: data.audio_info.ai_duration || audioRecordingData.aiDuration
                    };
                    updateAudioInfo();
                }
                
                addMessage('system', 'ğŸ“¥ Audio available for download and analysis');
            });

            // Audio conversation upload progress
            enhancedVAD.socket.on('audio_upload_progress', (data) => {
                log(`ğŸµ Audio upload progress: ${data.progress}%`);
                updateRecordingStatus(`Uploading ${data.progress}%`, 'warning');
            });

            // Audio conversation error
            enhancedVAD.socket.on('audio_recording_error', (data) => {
                log(`âŒ Audio recording error: ${data.error}`, 'error');
                addMessage('system', `âŒ Audio recording error: ${data.error}`);
                updateRecordingStatus('Error', 'error');
            });

            // Audio segment processed (real-time updates)
            enhancedVAD.socket.on('audio_segment_processed', (data) => {
                if (data.segment_type === 'user') {
                    onUserAudioSegment(data.duration || 3.0);
                } else if (data.segment_type === 'ai') {
                    onAIAudioSegment(data.duration || 2.0);
                }
            });

            log('âœ… Audio conversation event listeners set up');
        }

        function handleVideoAnalysisUpdate(data) {
            try {
                log(`ğŸ“Š Video analysis update received for session: ${data.session_id || 'unknown'}`);
                
                // Update video analysis state
                videoAnalysisActive = true;
                lastVideoAnalysisTime = Date.now();
                
                if (!analysis) {
                    log('âš ï¸ No analysis data in video update', 'warning');
                    return;
                }
                
                log(`ğŸ“ˆ Analysis data: hands=${analysis.hand_analysis?.hands_detected || 0}, emotion=${analysis.emotion_analysis?.dominant_emotion || 'none'}, pose=${analysis.pose_analysis?.pose_detected || false}`);
                
                const analysis = data.analysis;
                
                // Handle enhanced video analysis structure
                const handAnalysis = analysis.hand_analysis || {};
                const emotionAnalysis = analysis.emotion_analysis || {};
                const poseAnalysis = analysis.pose_analysis || {};
                const overallScores = analysis.overall_scores || {};
                
                if (data.analyzer_type === 'enhanced') {
                    // Handle enhanced video analysis data with correct field names
                    const handAnalysis = analysis.hand_analysis || {};
                    const emotionAnalysis = analysis.emotion_analysis || {};
                    const poseAnalysis = analysis.pose_analysis || {};
                    const overallScores = analysis.overall_scores || {};

                    // Update metrics with enhanced video analysis scores
                    // Map enhanced scores to UI metrics (gesture_score -> eye contact, etc.)
                    const gestureScore = Math.round((overallScores.gesture_score || 0.7) * 100);
                    const emotionScore = Math.round((overallScores.emotion_score || 0.75) * 100);
                    const postureScore = Math.round((overallScores.posture_score || 0.8) * 100);
                    const pitchReadiness = Math.round((overallScores.pitch_readiness || 0.72) * 100);
                    
                    // Update individual metrics with enhanced data
                    updateMetricScore('eyeContact', gestureScore); // Gesture effectiveness as eye contact proxy
                    updateMetricScore('engagement', postureScore); // Posture score as engagement
                    updateMetricScore('confidence', emotionScore); // Emotion score as confidence
                    updateMetricScore('clarity', pitchReadiness); // Pitch readiness as clarity
                    
                    // Calculate overall score from enhanced metrics
                    const overallScore = Math.round((gestureScore + emotionScore + postureScore + pitchReadiness) / 4);
                    updateMetricScore('overall', overallScore);

                    // Enhanced logging with detailed CVZone, FER, and MediaPipe data
                    if (handAnalysis.hands_detected > 0) {
                        const gestures = handAnalysis.gestures || [];
                        const gestureTypes = gestures.map(g => g.type || 'unknown').join(', ');
                        const effectiveness = Math.round((handAnalysis.gesture_effectiveness || 0) * 100);
                        log(`ğŸ‘‹ CVZone Hand Analysis: ${handAnalysis.hands_detected} hands detected, Gestures: [${gestureTypes}], Effectiveness: ${effectiveness}%`);
                        
                        // Update CVZone status
                        updateAnalysisStatus('cvzoneStatus', 'Active', 'success');
                        updateAnalysisStats('handsDetected', handAnalysis.hands_detected);
                        
                        // Update live gesture feedback
                        const gestureIcon = getGestureIcon(gestureTypes);
                        updateLiveFeedback('currentGesture', `${gestureIcon} ${gestureTypes || 'Hand detected'} (${effectiveness}% effective)`);
                    } else {
                        updateLiveFeedback('currentGesture', 'ğŸ‘‹ No gesture detected');
                    }
                    
                    if (emotionAnalysis.faces_detected > 0) {
                        const emotion = emotionAnalysis.dominant_emotion || 'neutral';
                        const confidence = Math.round((emotionAnalysis.confidence_score || 0) * 100);
                        const suitability = emotionAnalysis.pitch_suitability || 'moderate';
                        log(`ğŸ˜Š FER Emotion Analysis: ${emotion} (${confidence}% confidence), Pitch Suitability: ${suitability}`);
                        
                        // Update FER status
                        updateAnalysisStatus('ferStatus', `${emotion} (${confidence}%)`, 'success');
                        updateAnalysisStats('facesDetected', emotionAnalysis.faces_detected);
                        
                        // Update live emotion feedback
                        const emotionIcon = getEmotionIcon(emotion);
                        const suitabilityColor = getSuitabilityColor(suitability);
                        updateLiveFeedback('currentEmotion', `${emotionIcon} ${emotion} (${confidence}%) - ${suitability} for pitch`, suitabilityColor);
                    } else {
                        updateLiveFeedback('currentEmotion', 'ğŸ˜ No face detected');
                    }
                    
                    if (poseAnalysis.pose_detected) {
                        const engagement = poseAnalysis.engagement_level || 'neutral';
                        const posture = Math.round((poseAnalysis.posture_score || 0) * 100);
                        log(`ğŸ§ MediaPipe Pose Analysis: Engagement: ${engagement}, Posture Score: ${posture}%`);
                        
                        // Update MediaPipe status
                        updateAnalysisStatus('mediapipeStatus', `${engagement}`, 'success');
                        updateAnalysisStats('poseDetected', 'Yes');
                        
                        // Update live posture feedback
                        const postureIcon = getPostureIcon(engagement);
                        const engagementColor = getEngagementColor(engagement);
                        updateLiveFeedback('currentPosture', `${postureIcon} ${engagement} posture (${posture}% score)`, engagementColor);
                    } else {
                        updateLiveFeedback('currentPosture', 'ğŸ§ No pose detected');
                    }
                    
                    // Update analysis counter
                    let currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    updateAnalysisStats('analysisUpdates', currentUpdates + 1);

                    // Log and display AI recommendations from enhanced analysis
                    if (analysis.recommendations && analysis.recommendations.length > 0) {
                        const topRecommendation = analysis.recommendations[0];
                        log(`ğŸ’¡ Enhanced AI Recommendations: ${analysis.recommendations.slice(0, 2).join(', ')}`);
                        updateLiveFeedback('liveRecommendation', `ğŸ’¡ ${topRecommendation}`, 'text-blue-400');
                        
                        // Show recommendation in conversation occasionally
                        currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                        if (currentUpdates > 0 && currentUpdates % 15 === 0) {
                            addMessage('system', `ğŸ’¡ Enhanced AI Tip: ${topRecommendation}`);
                        }
                    }

                    // Comprehensive enhanced score logging
                    log(`ğŸ“Š Enhanced Video Analysis - Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Pitch Readiness: ${pitchReadiness}%, Overall: ${overallScore}%`);
                    
                    // Show periodic enhanced analysis summary in conversation
                    currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    if (currentUpdates > 0 && currentUpdates % 10 === 0) {
                        addMessage('system', `ğŸ“Š Enhanced Analysis Update #${currentUpdates}: Overall Score ${overallScore}% (Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Readiness: ${pitchReadiness}%)`);
                    }
                    
                    // Show periodic enhanced analysis summary in conversation
                    currentUpdates = parseInt(document.getElementById('analysisUpdates').textContent) || 0;
                    if (currentUpdates > 0 && currentUpdates % 10 === 0) {
                        addMessage('system', `ğŸ“Š Enhanced Analysis Update #${currentUpdates}: Overall Score ${overallScore}% (Gesture: ${gestureScore}%, Emotion: ${emotionScore}%, Posture: ${postureScore}%, Readiness: ${pitchReadiness}%)`);
                    }

                } else {
                    // Handle basic video analysis data
                    const hands = analysis.hands || {};
                    const face = analysis.face || {};
                    const pose = analysis.pose || {};

                    if (face.eye_contact_score !== undefined) {
                        updateMetricScore('eyeContact', Math.round(face.eye_contact_score * 100));
                    }
                    
                    if (pose.engagement_level) {
                        const engagementMap = {
                            'highly_engaged': 90,
                            'engaged': 70,
                            'neutral': 50,
                            'disengaged': 30
                        };
                        updateMetricScore('engagement', engagementMap[pose.engagement_level] || 50);
                    }
                }

            } catch (error) {
                log(`âŒ Error processing video analysis update: ${error.message}`, 'error');
            }
        }

        function handleVideoInsights(data) {
            const insights = data.insights || [];
            
            insights.forEach(insight => {
                // Enhanced insight categorization with appropriate icons
                let insightIcon = 'ğŸ’¡';
                let insightPrefix = 'Insight';
                
                if (insight.toLowerCase().includes('gesture') || insight.toLowerCase().includes('hand')) {
                    insightIcon = 'ğŸ‘‹';
                    insightPrefix = 'CVZone Hand Analysis';
                } else if (insight.toLowerCase().includes('emotion') || insight.toLowerCase().includes('expression') || insight.toLowerCase().includes('facial')) {
                    insightIcon = 'ğŸ˜Š';
                    insightPrefix = 'FER Emotion Analysis';
                } else if (insight.toLowerCase().includes('posture') || insight.toLowerCase().includes('pose') || insight.toLowerCase().includes('body')) {
                    insightIcon = 'ğŸ§';
                    insightPrefix = 'MediaPipe Pose Analysis';
                } else if (insight.toLowerCase().includes('confidence')) {
                    insightIcon = 'ğŸ’ª';
                    insightPrefix = 'Confidence Analysis';
                } else if (insight.toLowerCase().includes('engagement')) {
                    insightIcon = 'ğŸ¯';
                    insightPrefix = 'Engagement Analysis';
                } else if (insight.toLowerCase().includes('pitch') || insight.toLowerCase().includes('presentation')) {
                    insightIcon = 'ğŸš€';
                    insightPrefix = 'Pitch Analysis';
                }
                
                log(`${insightIcon} Enhanced ${insightPrefix}: ${insight}`);
                addMessage('system', `${insightIcon} ${insightPrefix}: ${insight}`);
            });
            
            // Log analysis source for debugging
            if (data.analyzer_type === 'enhanced') {
                log('ğŸ”¬ Enhanced insights generated using professional CV libraries');
            }
        }

        function updateAnalysisStatus(elementId, status, type) {
            const element = document.getElementById(elementId);
            if (element) {
                element.textContent = status;
                element.className = `px-2 py-1 rounded-full text-xs font-medium ${getStatusClass(type)}`;
            }
        }

        function updateAnalysisStats(statId, value) {
            const element = document.getElementById(statId);
            if (element) {
                element.textContent = value;
            }
        }

        function getStatusClass(type) {
            switch(type) {
                case 'success': return 'bg-green-600 text-white';
                case 'warning': return 'bg-yellow-600 text-white';
                case 'error': return 'bg-red-600 text-white';
                default: return 'bg-gray-600 text-white';
            }
        }

        function updateLiveFeedback(elementId, text, colorClass = '') {
            const element = document.getElementById(elementId);
            if (element) {
                element.textContent = text;
                if (colorClass) {
                    element.className = `text-white font-medium ${colorClass}`;
                }
            }
        }

        function getGestureIcon(gestureType) {
            if (!gestureType) return 'ğŸ‘‹';
            const type = gestureType.toLowerCase();
            if (type.includes('point')) return 'ğŸ‘‰';
            if (type.includes('open')) return 'âœ‹';
            if (type.includes('fist')) return 'âœŠ';
            if (type.includes('peace')) return 'âœŒï¸';
            if (type.includes('thumbs')) return 'ğŸ‘';
            return 'ğŸ‘‹';
        }

        function getEmotionIcon(emotion) {
            switch(emotion?.toLowerCase()) {
                case 'happy': return 'ğŸ˜Š';
                case 'confident': return 'ğŸ˜';
                case 'excited': return 'ğŸ˜ƒ';
                case 'focused': return 'ğŸ§';
                case 'calm': return 'ğŸ˜Œ';
                case 'surprised': return 'ğŸ˜®';
                case 'concerned': return 'ğŸ˜Ÿ';
                case 'nervous': return 'ğŸ˜°';
                case 'angry': return 'ğŸ˜ ';
                case 'sad': return 'ğŸ˜¢';
                default: return 'ğŸ˜';
            }
        }

        function getPostureIcon(engagement) {
            switch(engagement?.toLowerCase()) {
                case 'highly_engaged': return 'ğŸš€';
                case 'engaged': return 'ğŸ’ª';
                case 'neutral': return 'ğŸ§';
                case 'disengaged': return 'ğŸ˜´';
                default: return 'ğŸ§';
            }
        }

        function getSuitabilityColor(suitability) {
            switch(suitability?.toLowerCase()) {
                case 'excellent': return 'text-green-400';
                case 'good': return 'text-blue-400';
                case 'moderate': return 'text-yellow-400';
                case 'needs_improvement': return 'text-red-400';
                default: return 'text-white';
            }
        }

        function getEngagementColor(engagement) {
            switch(engagement?.toLowerCase()) {
                case 'highly_engaged': return 'text-green-400';
                case 'engaged': return 'text-blue-400';
                case 'neutral': return 'text-yellow-400';
                case 'disengaged': return 'text-red-400';
                default: return 'text-white';
            }
        }

        function updateMetricScore(metricType, score) {
            // Clamp score between 0 and 100
            score = Math.max(0, Math.min(100, score));
            
            const scoreElement = document.getElementById(`${metricType}Score`);
            const progressElement = document.getElementById(`${metricType}Progress`);
            
            if (scoreElement) {
                scoreElement.textContent = `${score}%`;
            }
            
            if (progressElement) {
                progressElement.style.width = `${score}%`;
                
                // Update color based on score
                let colorClass = 'bg-gradient-to-r ';
                if (score >= 80) {
                    colorClass += 'from-green-500 to-green-600';
                } else if (score >= 60) {
                    colorClass += 'from-yellow-500 to-yellow-600';
                } else if (score >= 40) {
                    colorClass += 'from-orange-500 to-orange-600';
                } else {
                    colorClass += 'from-red-500 to-red-600';
                }
                
                // Keep the existing color scheme for specific metrics
                if (metricType === 'eyeContact') {
                    colorClass = 'bg-gradient-to-r from-blue-500 to-blue-600';
                } else if (metricType === 'engagement') {
                    colorClass = 'bg-gradient-to-r from-green-500 to-green-600';
                } else if (metricType === 'confidence') {
                    colorClass = 'bg-gradient-to-r from-yellow-500 to-yellow-600';
                } else if (metricType === 'clarity') {
                    colorClass = 'bg-gradient-to-r from-purple-500 to-purple-600';
                } else if (metricType === 'overall') {
                    colorClass = 'bg-gradient-to-r from-indigo-500 to-indigo-600';
                }
                
                progressElement.className = `${colorClass} h-2 rounded-full transition-all duration-1000`;
                if (metricType === 'overall') {
                    progressElement.className = `${colorClass} h-3 rounded-full transition-all duration-1000`;
                }
            }
        }

        // UI Helper Functions
        function updateStatus(elementId, text, type = 'default') {
            const element = document.getElementById(elementId);
            if (!element) return;
            
            element.textContent = text;
            
            // Update status indicator if it exists
            const statusIndicator = element.previousElementSibling;
            if (statusIndicator && statusIndicator.classList.contains('w-3')) {
                let className = 'w-3 h-3 rounded-full mr-2 ' + getStatusColor(type);
                
                // Add animation for connecting/loading states
                if (type === 'warning' && (text.includes('Connecting') || text.includes('Initializing') || text.includes('Starting'))) {
                    className += ' animate-pulse';
                }
                
                statusIndicator.className = className;
            }
        }

        function updateConnectionStatus(text, type) {
            updateStatus('connectionStatusText', text, type);
        }

        function getStatusColor(type) {
            switch (type) {
                case 'success': return 'bg-green-500';
                case 'warning': return 'bg-yellow-500';
                case 'error': return 'bg-red-500';
                default: return 'bg-gray-500';
            }
        }

        function updateTranscriptionDisplay(data) {
            const display = document.getElementById('transcription-display');
            const status = document.getElementById('transcription-status');
            
            if (data.interim || data.final) {
                const text = data.interim || data.final;
                display.innerHTML = `<div class="text-white">${text}</div>`;
                status.textContent = data.final ? 'Processing...' : 'Listening...';
                status.className = data.final ? 'text-yellow-400 text-sm mb-2' : 'text-green-400 text-sm mb-2 animate-pulse';
            }
        }

        function addMessage(sender, message, hasAudio = false) {
            const conversationLog = document.getElementById('conversation-log');
            const messageDiv = document.createElement('div');
            
            let messageClass = 'p-3 rounded-lg ';
            let messageContent = '';
            
            if (sender === 'system') {
                messageClass += 'bg-blue-500/20 border border-blue-500/30';
                messageContent = `<div class="text-blue-300 text-sm">${message}</div>`;
            } else if (sender === 'user') {
                messageClass += 'bg-green-500/20 border border-green-500/30 ml-8';
                messageContent = `<div class="text-green-300"><strong>You:</strong> ${message}</div>`;
            } else if (sender === 'ai') {
                messageClass += 'bg-purple-500/20 border border-purple-500/30 mr-8';
                const audioIcon = hasAudio ? ' ğŸ”Š' : '';
                messageContent = `<div class="text-purple-300"><strong>AI Investor${audioIcon}:</strong> ${message}</div>`;
            }
            
            messageDiv.className = messageClass;
            messageDiv.innerHTML = messageContent;
            conversationLog.appendChild(messageDiv);
            conversationLog.scrollTop = conversationLog.scrollHeight;
        }

        // Audio Recording Functions
        let recordingStartTime = null;
        let recordingTimer = null;
        let currentAudioUrl = null;
        let audioRecordingData = {
            userSegments: 0,
            aiSegments: 0,
            userDuration: 0,
            aiDuration: 0
        };

        function startAudioRecording() {
            recordingStartTime = Date.now();
            updateRecordingStatus('Recording', 'success');
            
            // Start timer
            recordingTimer = setInterval(updateRecordingTimer, 1000);
            
            // Reset audio data
            audioRecordingData = {
                userSegments: 0,
                aiSegments: 0,
                userDuration: 0,
                aiDuration: 0
            };
            
            updateAudioInfo();
            log('ğŸµ Audio recording started');
        }

        function stopAudioRecording(audioUrl = null) {
            if (recordingTimer) {
                clearInterval(recordingTimer);
                recordingTimer = null;
            }
            
            updateRecordingStatus('Processing...', 'warning');
            
            if (audioUrl) {
                currentAudioUrl = audioUrl;
                showAudioPlayer(audioUrl);
                updateRecordingStatus('Completed', 'success');
                log('ğŸµ Audio recording completed with URL');
            } else {
                updateRecordingStatus('Ready to Record', 'default');
                log('ğŸµ Audio recording stopped');
            }
        }

        function updateRecordingStatus(status, type) {
            const statusElement = document.getElementById('recording-status');
            const indicatorElement = document.getElementById('recording-indicator');
            
            if (statusElement) {
                statusElement.textContent = status;
            }
            
            if (indicatorElement) {
                let className = 'w-3 h-3 rounded-full ';
                switch (type) {
                    case 'success':
                        className += 'bg-green-500';
                        if (status === 'Recording') {
                            className += ' animate-pulse';
                        }
                        break;
                    case 'warning':
                        className += 'bg-yellow-500 animate-pulse';
                        break;
                    case 'error':
                        className += 'bg-red-500';
                        break;
                    default:
                        className += 'bg-gray-500';
                }
                indicatorElement.className = className;
            }
        }

        function updateRecordingTimer() {
            if (!recordingStartTime) return;
            
            const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
            const minutes = Math.floor(elapsed / 60);
            const seconds = elapsed % 60;
            
            const durationElement = document.getElementById('recording-duration');
            if (durationElement) {
                durationElement.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
            }
        }

        function updateAudioInfo() {
            const userSegmentsElement = document.getElementById('user-segments');
            const userDurationElement = document.getElementById('user-duration');
            const aiSegmentsElement = document.getElementById('ai-segments');
            const aiDurationElement = document.getElementById('ai-duration');
            
            if (userSegmentsElement) {
                userSegmentsElement.textContent = `${audioRecordingData.userSegments} segments`;
            }
            if (userDurationElement) {
                userDurationElement.textContent = `${audioRecordingData.userDuration.toFixed(1)}s`;
            }
            if (aiSegmentsElement) {
                aiSegmentsElement.textContent = `${audioRecordingData.aiSegments} segments`;
            }
            if (aiDurationElement) {
                aiDurationElement.textContent = `${audioRecordingData.aiDuration.toFixed(1)}s`;
            }
        }

        function showAudioPlayer(audioUrl) {
            const playerContainer = document.getElementById('audio-player-container');
            const audioSource = document.getElementById('audio-source');
            const audioElement = document.getElementById('conversation-audio');
            
            if (playerContainer && audioSource && audioElement) {
                audioSource.src = audioUrl;
                audioElement.load();
                playerContainer.classList.remove('hidden');
                
                // Determine format from URL
                const formatElement = document.getElementById('audio-format');
                if (formatElement) {
                    const format = audioUrl.includes('.mp3') ? 'MP3' : 'WAV';
                    formatElement.textContent = format;
                }
            }
        }

        function downloadAudio() {
            if (currentAudioUrl) {
                const link = document.createElement('a');
                link.href = currentAudioUrl;
                link.download = `pitch-conversation-${sessionId || 'session'}.${currentAudioUrl.includes('.mp3') ? 'mp3' : 'wav'}`;
                document.body.appendChild(link);
                link.click();
                document.body.removeChild(link);
                
                addMessage('system', 'ğŸ“¥ Audio conversation downloaded');
                log('ğŸ“¥ Audio downloaded');
            }
        }

        function shareAudio() {
            if (currentAudioUrl) {
                if (navigator.share) {
                    navigator.share({
                        title: 'Pitch Practice Audio',
                        text: 'My AI pitch practice conversation',
                        url: currentAudioUrl
                    }).then(() => {
                        addMessage('system', 'ğŸ”— Audio shared successfully');
                    }).catch(err => {
                        copyToClipboard(currentAudioUrl);
                    });
                } else {
                    copyToClipboard(currentAudioUrl);
                }
            }
        }

        function copyToClipboard(text) {
            navigator.clipboard.writeText(text).then(() => {
                addMessage('system', 'ğŸ”— Audio URL copied to clipboard');
                log('ğŸ”— Audio URL copied to clipboard');
            }).catch(err => {
                addMessage('system', 'âŒ Failed to copy URL');
            });
        }

        function toggleAudioInAnalysis() {
            const button = document.getElementById('include-in-analysis-btn');
            if (button) {
                const isIncluded = button.textContent.includes('âœ…');
                if (isIncluded) {
                    button.innerHTML = 'ğŸ“Š Include in Analysis';
                    button.className = button.className.replace('bg-green-500/20 hover:bg-green-500/30 text-green-300', 'bg-purple-500/20 hover:bg-purple-500/30 text-purple-300');
                    addMessage('system', 'ğŸ“Š Audio removed from analysis');
                } else {
                    button.innerHTML = 'âœ… Included in Analysis';
                    button.className = button.className.replace('bg-purple-500/20 hover:bg-purple-500/30 text-purple-300', 'bg-green-500/20 hover:bg-green-500/30 text-green-300');
                    addMessage('system', 'ğŸ“Š Audio included in analysis');
                }
            }
        }

        // Update audio recording data when user speaks
        function onUserAudioSegment(duration) {
            audioRecordingData.userSegments++;
            audioRecordingData.userDuration += duration;
            updateAudioInfo();
        }

        // Update audio recording data when AI speaks
        function onAIAudioSegment(duration) {
            audioRecordingData.aiSegments++;
            audioRecordingData.aiDuration += duration;
            updateAudioInfo();
        }

        function updateRealtimeMetrics() {
            const now = Date.now();
            const timeSinceLastSpeech = now - userActivity.lastSpeechTime;
            const isRecentlySpeaking = timeSinceLastSpeech < 5000;
            
            // Check if we have recent video analysis data (within last 10 seconds)
            const hasRecentVideoData = videoAnalysisActive && (now - lastVideoAnalysisTime < 10000);
            
            if (hasRecentVideoData) {
                // Don't override metrics if we have recent video analysis data
                log('ğŸ“Š Using real video analysis data for metrics');
                return;
            }
            
            // Update metrics based on activity (fallback when no video data)
            const baseVariation = 8;
            
            // Eye contact
            const eyeContactTarget = isRecentlySpeaking ? 80 : 65;
            previousMetrics.eyeContact = smoothTransition(previousMetrics.eyeContact, eyeContactTarget, baseVariation, 40, 95);
            
            // Engagement
            const engagementTarget = isRecentlySpeaking ? 75 : 60;
            previousMetrics.engagement = smoothTransition(previousMetrics.engagement, engagementTarget, baseVariation, 30, 90);
            
            // Confidence
            const confidenceBonus = Math.min(userActivity.speechCount * 2, 15);
            const confidenceTarget = 55 + confidenceBonus + (isRecentlySpeaking ? 10 : 0);
            previousMetrics.confidence = smoothTransition(previousMetrics.confidence, confidenceTarget, 6, 35, 95);
            
            // Clarity
            const clarityTarget = 60 + Math.min(userActivity.speechCount * 1.5, 20);
            previousMetrics.clarity = smoothTransition(previousMetrics.clarity, clarityTarget, 8, 40, 90);
            
            // Overall
            previousMetrics.overall = (
                previousMetrics.eyeContact * 0.25 + 
                previousMetrics.engagement * 0.25 + 
                previousMetrics.confidence * 0.25 + 
                previousMetrics.clarity * 0.25
            );
            
            // Update UI
            updateMetricDisplay('eyeContact', previousMetrics.eyeContact);
            updateMetricDisplay('engagement', previousMetrics.engagement);
            updateMetricDisplay('confidence', previousMetrics.confidence);
            updateMetricDisplay('clarity', previousMetrics.clarity);
            updateMetricDisplay('overall', previousMetrics.overall);
        }

        function smoothTransition(current, target, variation, min, max) {
            const direction = target > current ? 1 : -1;
            const change = (Math.random() * variation) + (direction * variation * 0.3);
            const newValue = current + change;
            return Math.max(min, Math.min(max, newValue));
        }

        function updateMetricDisplay(metric, value) {
            const progressBar = document.getElementById(metric + 'Progress');
            const scoreText = document.getElementById(metric + 'Score');
            
            if (progressBar && scoreText) {
                progressBar.style.width = value + '%';
                scoreText.textContent = Math.round(value) + '%';
            }
        }

        // Quick Action Functions
        function forceStartListening() {
            if (!enhancedVAD || !sessionId) {
                addMessage('system', 'âŒ Start a session first');
                return;
            }
            
            try {
                enhancedVAD.resumeListening();
                updateStatus('audioStatusText', 'Listening', 'success');
                document.getElementById('transcription-status').textContent = 'Listening... Start speaking!';
                document.getElementById('transcription-status').className = 'text-green-400 text-sm mb-2 animate-pulse';
                addMessage('system', 'ğŸ¤ Speech recognition started');
            } catch (error) {
                addMessage('system', `âŒ Error: ${error.message}`);
            }
        }

        function testTTS() {
            const testMessage = "Hello! This is a test of the text-to-speech system. I'm ready to help you practice your pitch!";
            
            if (enhancedVAD && enhancedVAD.config.ttsEnabled) {
                enhancedVAD.speakAIResponse(testMessage);
                addMessage('system', 'ğŸ§ª Testing TTS - you should hear audio now');
            } else {
                addMessage('system', 'âŒ TTS is disabled or system not ready');
            }
        }

        function debugAudioStreaming() {
            log('ğŸ” Audio Streaming Debug Info:');
            
            if (!enhancedVAD) {
                log('âŒ EnhancedVAD not initialized');
                addMessage('system', 'âŒ EnhancedVAD not initialized');
                return;
            }
            
            log(`ğŸµ Audio Streaming Status:`);
            log(`   - Streaming Enabled: ${enhancedVAD.audioStreamingEnabled}`);
            log(`   - Audio Context: ${enhancedVAD.audioContext ? enhancedVAD.audioContext.state : 'null'}`);
            log(`   - Media Stream: ${enhancedVAD.mediaStream ? 'active' : 'null'}`);
            log(`   - Audio Processor: ${enhancedVAD.audioProcessor ? 'connected' : 'null'}`);
            log(`   - Audio Chunks Sent: ${enhancedVAD.audioChunkCount || 0}`);
            log(`   - Session ID: ${enhancedVAD.currentSessionId || 'none'}`);
            log(`   - Is Recording: ${enhancedVAD.isRecording}`);
            log(`   - AI Speaking: ${enhancedVAD.isAISpeaking}`);
            
            if (enhancedVAD.mediaStream) {
                const tracks = enhancedVAD.mediaStream.getAudioTracks();
                log(`   - Audio Tracks: ${tracks.length}`);
                tracks.forEach((track, i) => {
                    log(`     Track ${i}: ${track.label} (${track.readyState})`);
                });
            }
            
            addMessage('system', 'ğŸ” Audio streaming debug info logged to console');
            
            // Test audio context resume
            if (enhancedVAD.audioContext && enhancedVAD.audioContext.state === 'suspended') {
                log('ğŸ”§ Attempting to resume audio context...');
                enhancedVAD.audioContext.resume().then(() => {
                    log('âœ… Audio context resumed');
                    addMessage('system', 'âœ… Audio context resumed');
                }).catch(error => {
                    log(`âŒ Failed to resume audio context: ${error}`);
                    addMessage('system', `âŒ Failed to resume audio context: ${error.message}`);
                });
            }
        }

        function testAudioStreaming() {
            if (!enhancedVAD) {
                addMessage('system', 'âŒ System not initialized');
                return;
            }
            
            log('ğŸ§ª Testing audio streaming...');
            addMessage('system', 'ğŸ§ª Testing audio streaming - speak now!');
            
            // Force start audio streaming
            enhancedVAD.forceStartAudioStreaming().then(success => {
                if (success) {
                    log('âœ… Audio streaming test started');
                    addMessage('system', 'âœ… Audio streaming ready - speak to test');
                    
                    // Monitor for 10 seconds
                    const startCount = enhancedVAD.audioChunkCount || 0;
                    setTimeout(() => {
                        const endCount = enhancedVAD.audioChunkCount || 0;
                        const chunksSent = endCount - startCount;
                        log(`ğŸ“Š Audio chunks sent in 10 seconds: ${chunksSent}`);
                        addMessage('system', `ğŸ“Š Audio chunks sent: ${chunksSent} (should be >0 if speaking)`);
                    }, 10000);
                } else {
                    log('âŒ Audio streaming test failed');
                    addMessage('system', 'âŒ Audio streaming test failed');
                }
            });
        }

        function debugSystem() {
            log('ğŸ” Running system debug...');
            addMessage('system', 'ğŸ” System Debug Information:');
            
            // Browser detection
            const isEdge = navigator.userAgent.indexOf('Edg') !== -1;
            const isChrome = navigator.userAgent.indexOf('Chrome') !== -1 && !isEdge;
            const isSafari = navigator.userAgent.indexOf('Safari') !== -1 && !isChrome && !isEdge;
            const isFirefox = navigator.userAgent.indexOf('Firefox') !== -1;
            
            addMessage('system', `ğŸŒ Browser: ${isEdge ? 'Microsoft Edge' : isChrome ? 'Chrome' : isSafari ? 'Safari' : isFirefox ? 'Firefox' : 'Unknown'}`);
            addMessage('system', `ğŸ“± User Agent: ${navigator.userAgent}`);
            
            // Check dependencies
            addMessage('system', `Socket.IO: ${typeof io !== 'undefined' ? 'âœ… Loaded' : 'âŒ Not loaded'}`);
            addMessage('system', `EnhancedHybridVAD: ${typeof EnhancedHybridVAD !== 'undefined' ? 'âœ… Loaded' : 'âŒ Not loaded'}`);
            
            // Check browser capabilities with Edge-specific details
            addMessage('system', `getUserMedia: ${navigator.mediaDevices && navigator.mediaDevices.getUserMedia ? 'âœ… Available' : 'âŒ Not available'}`);
            addMessage('system', `WebRTC: ${window.RTCPeerConnection ? 'âœ… Available' : 'âŒ Not available'}`);
            
            // Speech API with Edge-specific checks
            const hasSpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const hasWebkitSpeech = window.webkitSpeechRecognition;
            const hasStandardSpeech = window.SpeechRecognition;
            
            addMessage('system', `Web Speech API: ${hasSpeechRecognition ? 'âœ… Available' : 'âŒ Not available'}`);
            if (isEdge) {
                addMessage('system', `  â€¢ Standard SpeechRecognition: ${hasStandardSpeech ? 'âœ…' : 'âŒ'}`);
                addMessage('system', `  â€¢ Webkit SpeechRecognition: ${hasWebkitSpeech ? 'âœ…' : 'âŒ'}`);
            }
            
            // TTS capabilities (Server-side Google Cloud TTS only)
            addMessage('system', `Text-to-Speech: ğŸŒ Google Cloud TTS (Server-side)`);
            
            // Audio Context with Edge-specific checks
            const hasAudioContext = window.AudioContext || window.webkitAudioContext;
            addMessage('system', `AudioContext: ${hasAudioContext ? 'âœ… Available' : 'âŒ Not available'}`);
            if (isEdge) {
                addMessage('system', `  â€¢ Standard AudioContext: ${window.AudioContext ? 'âœ…' : 'âŒ'}`);
                addMessage('system', `  â€¢ Webkit AudioContext: ${window.webkitAudioContext ? 'âœ…' : 'âŒ'}`);
            }
            
            // Check current state
            addMessage('system', `Enhanced VAD Instance: ${enhancedVAD ? 'âœ… Created' : 'âŒ Not created'}`);
            addMessage('system', `Session ID: ${sessionId || 'None'}`);
            
            // Video analysis specific debug
            addMessage('system', `Video Analysis Active: ${videoAnalysisActive ? 'âœ… Active' : 'âŒ Inactive'}`);
            addMessage('system', `Last Video Analysis: ${lastVideoAnalysisTime ? new Date(lastVideoAnalysisTime).toLocaleTimeString() : 'âŒ Never'}`);
            addMessage('system', `Video Element Ready: ${document.getElementById('videoPreview')?.readyState >= 2 ? 'âœ… Ready' : 'âŒ Not ready'}`);
            addMessage('system', `Video Analysis Interval: ${videoAnalysisInterval ? 'âœ… Running' : 'âŒ Not running'}`);
            
            // Edge-specific recommendations
            if (isEdge) {
                addMessage('system', 'ğŸŒ Edge-specific recommendations:');
                addMessage('system', 'â€¢ Ensure Edge is updated to latest version');
                addMessage('system', 'â€¢ Check microphone permissions in Edge settings');
                addMessage('system', 'â€¢ Try clearing browser cache if issues persist');
                addMessage('system', 'â€¢ Disable ad blockers that might interfere with WebRTC');
            }
            
            // Log to console for detailed debugging
            console.log('ğŸ” Debug Info:', {
                browser: isEdge ? 'Edge' : isChrome ? 'Chrome' : isSafari ? 'Safari' : isFirefox ? 'Firefox' : 'Unknown',
                io: typeof io,
                EnhancedHybridVAD: typeof EnhancedHybridVAD,
                enhancedVAD: enhancedVAD,
                sessionId: sessionId,
                videoAnalysisActive: videoAnalysisActive,
                lastVideoAnalysisTime: lastVideoAnalysisTime,
                mediaStream: mediaStream,
                userAgent: navigator.userAgent,
                location: window.location.href,
                speechAPIs: {
                    SpeechRecognition: !!window.SpeechRecognition,
                    webkitSpeechRecognition: !!window.webkitSpeechRecognition,
                    ttsMode: 'Google Cloud TTS (Server-side only)'
                },
                audioAPIs: {
                    AudioContext: !!window.AudioContext,
                    webkitAudioContext: !!window.webkitAudioContext,
                    getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)
                }
            });
        }
        
        // Add this function to test video analysis specifically
        function debugVideoAnalysis() {
            log('ğŸ¥ Debugging video analysis...');
            addMessage('system', 'ğŸ¥ Video Analysis Debug:');
            
            const video = document.getElementById('videoPreview');
            addMessage('system', `Video Element: ${video ? 'âœ… Found' : 'âŒ Not found'}`);
            addMessage('system', `Video Ready State: ${video ? video.readyState : 'N/A'} (need â‰¥2)`);
            addMessage('system', `Video Dimensions: ${video ? `${video.videoWidth}x${video.videoHeight}` : 'N/A'}`);
            addMessage('system', `Media Stream: ${mediaStream ? 'âœ… Active' : 'âŒ Not active'}`);
            addMessage('system', `Video Analysis Active: ${videoAnalysisActive ? 'âœ… Active' : 'âŒ Inactive'}`);
            
            if (sessionId && enhancedVAD && enhancedVAD.socket) {
                addMessage('system', 'ğŸ§ª Testing frame capture...');
                try {
                    captureAndSendFrame();
                    addMessage('system', 'âœ… Frame capture test completed - check server logs');
                } catch (error) {
                    addMessage('system', `âŒ Frame capture test failed: ${error.message}`);
                }
            } else {
                addMessage('system', 'âŒ Cannot test frame capture: missing session or socket');
            }
        }

        async function startSession() {
            log('ğŸš€ Starting practice session...');
            addMessage('system', 'ğŸš€ Starting practice session...');
            
            if (!enhancedVAD) {
                addMessage('system', 'âŒ Enhanced VAD not initialized');
                return;
            }
            
            const persona = document.getElementById('personaSelect').value;
            
            // Generate session ID
            const newSessionId = 'session_' + Date.now() + '_' + Math.random().toString(36).substr(2, 9);
            
            try {
                // Start the session
                await enhancedVAD.startSession(newSessionId, persona);
                
                // Update global session ID
                sessionId = newSessionId;
                
                // Update UI
                document.getElementById('start-btn').disabled = true;
                document.getElementById('stop-btn').disabled = false;
                
                addMessage('system', `âœ… Session started with ${persona} investor persona`);
                addMessage('system', `ğŸ“‹ Session ID: ${newSessionId}`);
            } catch (error) {
                addMessage('system', `âŒ Failed to start session: ${error.message}`);
                log(`Start session error: ${error.message}`, 'error');
            }
        }
        
        async function stopSession() {
            log('â¹ï¸ Stopping practice session...');
            addMessage('system', 'â¹ï¸ Stopping practice session...');
            
            if (!enhancedVAD) {
                addMessage('system', 'âŒ Enhanced VAD not initialized');
                return;
            }
            
            try {
                // Stop the session
                await enhancedVAD.stopSession();
                
                // Clear global session ID
                sessionId = null;
                
                // Update UI
                document.getElementById('start-btn').disabled = false;
                document.getElementById('stop-btn').disabled = true;
                
                addMessage('system', 'âœ… Session stopped');
            } catch (error) {
                addMessage('system', `âŒ Failed to stop session: ${error.message}`);
                log(`Stop session error: ${error.message}`, 'error');
            }
        }

        function retryConnection() {
            log('ğŸ”„ Retrying connection...');
            addMessage('system', 'ğŸ”„ Retrying connection...');
            
            // Reset state
            enhancedVAD = null;
            sessionId = null;
            
            // Hide retry button
            document.getElementById('retry-connection-btn').style.display = 'none';
            
            // Retry initialization
            initializeSystem();
        }

        // Test functions for debugging
        function forceStartListening() {
            log('ğŸ¤ Force starting listening...');
            addMessage('system', 'ğŸ¤ Attempting to force start listening...');
            
            if (enhancedVAD && enhancedVAD.startListening) {
                enhancedVAD.startListening();
                addMessage('system', 'âœ… Listening started');
            } else {
                addMessage('system', 'âŒ Enhanced VAD not available');
            }
        }



        function testTTS() {
            log('ğŸŒ Testing Google Cloud TTS (Server-side only)...');
            addMessage('system', 'ğŸŒ Testing Google Cloud TTS...');
            
        }

        function debugAudioStreaming() {
            log('ğŸ” Debugging audio streaming...');
            addMessage('system', 'ğŸ” Audio streaming debug info:');
            
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    addMessage('system', 'âœ… Audio access granted');
                    stream.getTracks().forEach(track => track.stop());
                })
                .catch(error => {
                    addMessage('system', `âŒ Audio access failed: ${error.message}`);
                });
        }

        async function testServerTTS() {
            log('ğŸŒ Testing server-side Google Cloud TTS...');
            addMessage('system', 'ğŸŒ Testing server-side Google Cloud TTS...');
            
            try {
                const response = await fetch('/api/tts/test?text=Hello! This is a test of the Google Cloud Text-to-Speech system.&persona=friendly');
                const data = await response.json();
                
                if (data.success && data.audio_data) {
                    addMessage('system', 'âœ… Server TTS generated successfully');
                    addMessage('system', `ğŸ“Š Audio size: ${data.audio_size} bytes`);
                    
                    // Play the server-generated audio
                    if (enhancedVAD && enhancedVAD.playServerTTSAudio) {
                        addMessage('system', 'ğŸ”Š Playing server TTS audio...');
                        await enhancedVAD.playServerTTSAudio(data.audio_data, data.text);
                        addMessage('system', 'âœ… Server TTS playback completed');
                    } else {
                        // Fallback: create audio element directly
                        const audioBytes = atob(data.audio_data);
                        const audioArray = new Uint8Array(audioBytes.length);
                        for (let i = 0; i < audioBytes.length; i++) {
                            audioArray[i] = audioBytes.charCodeAt(i);
                        }
                        
                        const audioBlob = new Blob([audioArray], { type: 'audio/mp3' });
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        
                        audio.onended = () => {
                            URL.revokeObjectURL(audioUrl);
                            addMessage('system', 'âœ… Server TTS playback completed');
                        };
                        
                        audio.onerror = (error) => {
                            addMessage('system', `âŒ Audio playback failed: ${error.message || 'Unknown error'}`);
                            URL.revokeObjectURL(audioUrl);
                        };
                        
                        addMessage('system', 'ğŸ”Š Playing server TTS audio...');
                        await audio.play();
                    }
                } else {
                    addMessage('system', `âŒ Server TTS failed: ${data.detail || 'Unknown error'}`);
                }
            } catch (error) {
                addMessage('system', `âŒ Server TTS test failed: ${error.message}`);
                log(`Server TTS test error: ${error.message}`, 'error');
            }
        }

        function testAudioStreaming() {
            log('ğŸµ Testing audio streaming...');
            addMessage('system', 'ğŸµ Testing audio streaming...');
            
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    addMessage('system', 'âœ… Audio stream test successful');
                    setTimeout(() => {
                        stream.getTracks().forEach(track => track.stop());
                        addMessage('system', 'ğŸ”‡ Audio stream test completed');
                    }, 2000);
                })
                .catch(error => {
                    addMessage('system', `âŒ Audio stream test failed: ${error.message}`);
                });
        }

        function log(message, level = 'info') {
            console.log(`[MultimodalDemo] ${message}`);
        }

        // Initialize enhanced analysis information
        function showEnhancedAnalysisInfo() {
            addMessage('system', 'ğŸš€ Enhanced Multimodal Pitch Analysis Ready!');
            addMessage('system', 'ğŸ”¬ Professional Computer Vision Libraries Integrated:');
            addMessage('system', 'ğŸ‘‹ CVZone: Advanced hand gesture recognition with effectiveness scoring');
            addMessage('system', 'ğŸ˜Š FER: Facial emotion recognition with pitch suitability analysis');
            addMessage('system', 'ğŸ§ MediaPipe: Professional pose analysis and engagement detection');
            addMessage('system', 'ğŸ“Š Real-time metrics update based on your actual performance');
            addMessage('system', 'ğŸ¯ Live feedback shows current gestures, emotions, and posture');
            addMessage('system', 'ğŸ’¡ AI recommendations generated from video analysis insights');
            addMessage('system', 'ğŸš€ Click "Start Practice Session" to begin your enhanced pitch analysis!');
        }

        // Initialize on page load
        window.addEventListener('load', function() {
            setTimeout(showEnhancedAnalysisInfo, 1000);
        });

        // Cleanup on page unload
        window.addEventListener('beforeunload', function() {
            if (enhancedVAD) {
                enhancedVAD.cleanup();
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
            }
        });
    </script>
</body>
</html>